<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
<front>
<journal-meta><journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="publisher-id">08-PONE-RA-06856</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0003892</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Neuroscience</subject><subject>Neuroscience/Cognitive Neuroscience</subject><subject>Neuroscience/Sensory Systems</subject></subj-group></article-categories><title-group><article-title>Decoding Face Information in Time, Frequency and Space from Direct Intracranial Recordings of the Human Brain</article-title><alt-title alt-title-type="running-head">Decoding Faces in ECoG</alt-title></title-group><contrib-group>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple"><name name-style="western"><surname>Tsuchiya</surname><given-names>Naotsugu</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple"><name name-style="western"><surname>Kawasaki</surname><given-names>Hiroto</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Oya</surname><given-names>Hiroyuki</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Howard</surname><given-names>Matthew A.</given-names><suffix>III</suffix></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Adolphs</surname><given-names>Ralph</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff4"><sup>4</sup></xref></contrib>
</contrib-group><aff id="aff1"><label>1</label><addr-line>Division of Humanities and Social Sciences, Caltech, Pasadena, California, United States of America</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>Department of Neurosurgery, University of Iowa, Iowa City, Iowa, United States of America</addr-line>       </aff><aff id="aff3"><label>3</label><addr-line>Department of Neurology, University of Iowa, Iowa City, Iowa, United States of America</addr-line>       </aff><aff id="aff4"><label>4</label><addr-line>Division of Biology, Caltech, Pasadena, California, United States of America</addr-line>       </aff><contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Lauwereyns</surname><given-names>Jan</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group><aff id="edit1">Victoria University of Wellington, New Zealand</aff><author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">naotsu@gmail.com</email></corresp>
<fn fn-type="con"><p>Conceived and designed the experiments: HK RA. Performed the experiments: HK. Analyzed the data: NT. Contributed reagents/materials/analysis tools: NT. Wrote the paper: NT HK RA. Initial data analyses: HK. Performed anatomical localization of the electrodes: HO. Performed the neurosurgery and oversaw all recordings: MAH.</p></fn>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="collection"><year>2008</year></pub-date><pub-date pub-type="epub"><day>9</day><month>12</month><year>2008</year></pub-date><volume>3</volume><issue>12</issue><elocation-id>e3892</elocation-id><history>
<date date-type="received"><day>14</day><month>10</month><year>2008</year></date>
<date date-type="accepted"><day>6</day><month>11</month><year>2008</year></date>
</history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2008</copyright-year><copyright-holder>Tsuchiya et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
<p>Faces are processed by a neural system with distributed anatomical components, but the roles of these components remain unclear. A dominant theory of face perception postulates independent representations of invariant aspects of faces (e.g., identity) in ventral temporal cortex including the fusiform gyrus, and changeable aspects of faces (e.g., emotion) in lateral temporal cortex including the superior temporal sulcus. Here we recorded neuronal activity directly from the cortical surface in 9 neurosurgical subjects undergoing epilepsy monitoring while they viewed static and dynamic facial expressions. Applying novel decoding analyses to the power spectrogram of electrocorticograms (ECoG) from over 100 contacts in ventral and lateral temporal cortex, we found better representation of both invariant and changeable aspects of faces in ventral than lateral temporal cortex. Critical information for discriminating faces from geometric patterns was carried by power modulations between 50 to 150 Hz. For both static and dynamic face stimuli, we obtained a higher decoding performance in ventral than lateral temporal cortex. For discriminating fearful from happy expressions, critical information was carried by power modulation between 60–150 Hz and below 30 Hz, and again better decoded in ventral than lateral temporal cortex. Task-relevant attention improved decoding accuracy more than10% across a wide frequency range in ventral but not at all in lateral temporal cortex. Spatial searchlight decoding showed that decoding performance was highest around the middle fusiform gyrus. Finally, we found that the right hemisphere, in general, showed superior decoding to the left hemisphere. Taken together, our results challenge the dominant model for independent face representation of invariant and changeable aspects: information about both face attributes was better decoded from a single region in the middle fusiform gyrus.</p>
</abstract><funding-group><funding-statement>This work was supported by a fellowship from the Japan Society for the Promotion of Science (N.T.) and grants from NIH (R03 MH070497-01A2 to H.K.; R01 DC004290-06 to M.H.), the James S. McDonnell Foundation (R.A.) and the Gordon and Betty Moore Foundation (R.A.). The funders had no role in study design, data collection and analysis, decision to publish or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="17"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Faces are processed by a relatively dedicated but anatomically distributed system. This proposition has received strong convergent support from intracranial recordings in humans <xref ref-type="bibr" rid="pone.0003892-Allison1">[1]</xref>–<xref ref-type="bibr" rid="pone.0003892-Mitra1">[5]</xref>, <xref ref-type="bibr" rid="pone.0003892-Pesaran1">[6]</xref>, <xref ref-type="bibr" rid="pone.0003892-Poggio1">[7]</xref>, <xref ref-type="bibr" rid="pone.0003892-Hung1">[8]</xref> as well as from a large number of imaging studies <xref ref-type="bibr" rid="pone.0003892-Macmillan1">[9]</xref>–<xref ref-type="bibr" rid="pone.0003892-Nir2">[12]</xref>, scalp EEG <xref ref-type="bibr" rid="pone.0003892-Mitra1">[5]</xref>, <xref ref-type="bibr" rid="pone.0003892-Haxby1">[13]</xref>–<xref ref-type="bibr" rid="pone.0003892-Calder1">[17]</xref> and MEG studies <xref ref-type="bibr" rid="pone.0003892-DeRenzi1">[18]</xref>, <xref ref-type="bibr" rid="pone.0003892-Adolphs1">[19]</xref>, in addition to lesion studies in humans <xref ref-type="bibr" rid="pone.0003892-McCarthy2">[20]</xref>, <xref ref-type="bibr" rid="pone.0003892-Fried1">[21]</xref> and neurophysiological studies in monkeys <xref ref-type="bibr" rid="pone.0003892-Morris1">[22]</xref>–<xref ref-type="bibr" rid="pone.0003892-LaBar1">[25]</xref>. While debates about the modularity of face processing continue <xref ref-type="bibr" rid="pone.0003892-Sato1">[26]</xref>, <xref ref-type="bibr" rid="pone.0003892-Bentin1">[27]</xref> there is consensus in the notion of a face-processing system that encompasses specific sectors of temporal visual cortex.</p>
<p>Distinct facial attributes, such as emotional expression, gender, and identity, are extracted through this face processing system in partly segregated functional streams <xref ref-type="bibr" rid="pone.0003892-Seeck1">[28]</xref>–<xref ref-type="bibr" rid="pone.0003892-Pizzagalli1">[31]</xref>. In particular, it is thought that while static aspects of a face, such as its gender and identity, are encoded primarily in the ventral temporal regions, dynamic information, such as emotional expression, depends on the lateral and superior regions in the superior temporal sulcus and gyrus <xref ref-type="bibr" rid="pone.0003892-Macmillan1">[9]</xref>, <xref ref-type="bibr" rid="pone.0003892-Kanwisher1">[24]</xref>, <xref ref-type="bibr" rid="pone.0003892-LaBar1">[25]</xref>, <xref ref-type="bibr" rid="pone.0003892-Seeck1">[28]</xref>. This functional division of labor also meshes well with a dominant and influential model of face processing, which argues that faces need to be identified regardless of their expression, and that emotional expressions must often be recognized across different identities. Based in large part on this idea as well as behavioral data, the model proposes that identity and emotional expression information are processed by separate systems <xref ref-type="bibr" rid="pone.0003892-Liu1">[32]</xref>. Recently, functional imaging data has buttressed this model, suggesting that invariant aspects of faces, including identity, are represented in the fusiform face area (FFA) <xref ref-type="bibr" rid="pone.0003892-Logothetis1">[10]</xref>, <xref ref-type="bibr" rid="pone.0003892-Halgren2">[33]</xref>, in the ventral temporal cortex, while changeable aspects of faces, including emotional expressions, are represented in regions around the superior temporal sulcus (STS) <xref ref-type="bibr" rid="pone.0003892-Seeck1">[28]</xref>. However, a recent update to this model argues that there is early common processing of invariant and changeable facial attributes within the ventral temporal cortex, whose outputs are then conveyed to multiple cortical regions for further processing of distinct attributes <xref ref-type="bibr" rid="pone.0003892-MouchetantRostaing1">[30]</xref>, <xref ref-type="bibr" rid="pone.0003892-KrolakSalmon1">[34]</xref>. Given these alternative hypotheses, it is of special interest to contrast the information represented within the ventral temporal cortex with that represented in the lateral temporal cortex, and to examine the issue at a more precise resolution in time and frequency.</p>
<p>Just as information about faces is spatially distributed across cortical sites <xref ref-type="bibr" rid="pone.0003892-Macmillan1">[9]</xref>, <xref ref-type="bibr" rid="pone.0003892-Nir1">[11]</xref>, <xref ref-type="bibr" rid="pone.0003892-Nir2">[12]</xref>, <xref ref-type="bibr" rid="pone.0003892-Morris1">[22]</xref>, <xref ref-type="bibr" rid="pone.0003892-Seeck1">[28]</xref>, faces are processed at various temporal scales. Event-related potentials (ERP) measured by scalp EEG and MEG have shown that the visual system classifies a stimulus category rapidly within around 100 msec based on the visual characteristics of the input <xref ref-type="bibr" rid="pone.0003892-Lachaux1">[35]</xref>, <xref ref-type="bibr" rid="pone.0003892-Bruce1">[36]</xref>. Faces, in particular, evoke activity in the fusiform gyri at around 170 msec, reflecting more detailed processing about various aspects of faces <xref ref-type="bibr" rid="pone.0003892-Allison1">[1]</xref>–<xref ref-type="bibr" rid="pone.0003892-Puce1">[3]</xref>, <xref ref-type="bibr" rid="pone.0003892-Mitra1">[5]</xref>, <xref ref-type="bibr" rid="pone.0003892-Hung1">[8]</xref>, <xref ref-type="bibr" rid="pone.0003892-Haxby1">[13]</xref>–<xref ref-type="bibr" rid="pone.0003892-Adolphs1">[19]</xref>. Further face processing includes cognitive and emotional evaluation, linking conceptual knowledge signaled by the faces. Such later face processing would involve many subcortical structures such as amygdala, basal ganglia, hypothalamus, brain stem, as well as cortical areas such as orbitofrontal, somatosensory, and insular cortices <xref ref-type="bibr" rid="pone.0003892-Poggio1">[7]</xref>, <xref ref-type="bibr" rid="pone.0003892-Pizzagalli1">[31]</xref>. These prior findings leave open several important questions: Exactly what aspect of faces is encoded at early and at late latencies? Which regions of cortex participate in such encoding? And how does information flow from one region to another within the network?</p>
<p>The spatiotemporal complexity of face processing poses methodological difficulties in obtaining rich descriptions of how, and what point in time, different regions represent facial information. Moderately good spatial resolution and very wide field-of-view can be attained using fMRI, yet temporal resolution is limited to the timescale of seconds. Millisecond temporal resolution obtained using scalp EEG and MEG, on the other hand, is limited in terms of spatial resolution. Although direct single-unit recordings offer the best possible spatio-temporal resolution in principle, this technique suffers from an extremely narrow anatomical field-of-view together with very rare opportunities to obtain such recordings in humans <xref ref-type="bibr" rid="pone.0003892-Kanwisher2">[37]</xref>–<xref ref-type="bibr" rid="pone.0003892-Sugase1">[40]</xref>. Most importantly, none of these approaches provides a wide bandwidth such that different frequency components of processing could be adequately examined. Arguably the best combination of large anatomical field-of-view, good spatial resolution, excellent temporal resolution, and wide frequency bandwidth, is afforded by field potentials, which can be recorded in awake neurosurgical subjects <xref ref-type="bibr" rid="pone.0003892-Allison1">[1]</xref>–<xref ref-type="bibr" rid="pone.0003892-Halgren1">[4]</xref>, <xref ref-type="bibr" rid="pone.0003892-Pesaran1">[6]</xref>–<xref ref-type="bibr" rid="pone.0003892-Hung1">[8]</xref>, <xref ref-type="bibr" rid="pone.0003892-Young1">[41]</xref>–<xref ref-type="bibr" rid="pone.0003892-Foxe1">[44]</xref>.</p>
<p>To take a closer look at the face processing system in space, time and across frequency bands, we recorded intracranial multi-channel electrocorticograms (ECoG) from 9 subjects, who were performing a discrimination task on static and dynamic face stimuli (<xref ref-type="fig" rid="pone-0003892-g001">Figure 1</xref>). We analyzed the ECoG using a time-frequency decomposition. Time-frequency analyses allow much better preservation of information than the conventional event-related potential (ERP) of the raw ECoG (<xref ref-type="supplementary-material" rid="pone.0003892.s001">Figure S1</xref> and <xref ref-type="supplementary-material" rid="pone.0003892.s002">S2</xref>), yet also introduce three large challenges. First, the data are high-dimensional (amplitude values defined at different time points at different frequencies in many channel locations). Second, the many concurrent recording channels require statistical corrections for multiple comparisons that severely limit statistical power. And third, inter-subject variation in electrode locations and the most responsive frequency ranges makes population-level inferences problematic, since it is unclear how to pool data across multiple subjects.</p>
<fig id="pone-0003892-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0003892.g001</object-id><label>Figure 1</label><caption>
<title>Experimental Paradigm.</title>
<p>A We presented three kinds of morphing stimuli; 1) neutral to happy (80 trials), 2) neutral to fear (80 trials), 3) radial checkerboard (40 trials), the order of which was randomized within a session of 200 trials. A trial started with the baseline plaid pattern (−1&lt;t&lt;0). At <italic>t</italic> = 0, either a neutral face or a checkerboard was presented. At <italic>t</italic> = 1, the face started to morph into either fearful or happy, or the checkerboard expanded or contracted. After <italic>t</italic>&gt;1.5, the stimuli remained frozen and a response was prompted at <italic>t</italic> = 2.5. Subjects performed either an emotion- or a gender- discrimination task with three alternatives (see main text) in a given session. 9 subjects were studied in 22 sessions. B and C Distribution of the electrodes in the ventral (B) and the lateral (C) cortex. The electrode placement for each subject is shown with a different symbol and color, superimposed on one representative subject's brain surface. D and E The electrode density map, representing the frequency of the electrode placement for all subjects. A faint outline of the brain is superimposed. See <xref ref-type="sec" rid="s4">Methods</xref> for details of how the density map was computed.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0003892.g001" xlink:type="simple"/></fig>
<p>To address these problems, we applied a decoding approach to our time-frequency decomposed data. For example, combining the data across channels and frequencies within a subject, we can assess when information for emotion discrimination becomes available, effectively reducing the dimensions of the data and alleviating the multiple comparison problem. An optimal combination across channels blurs the exact locations of electrodes, solving the problem of inter-subject variation in sensitive electrode location and responsive frequencies.</p>
<p>Using this decoding approach, we were able to compare information processing in the ventral and the lateral temporal cortex. We found that higher decoding accuracy was obtained in the ventral than the lateral temporal cortex when we tried to discriminate faces from checkerboard patterns and fearful from happy expressions. Decoding time-frequency maps revealed critical frequency bands of 50–150 Hz for discrimination of faces from checkerboards not only when the stimuli were static, but also when the stimuli were morphing; in both cases, the decoding accuracy was better in the ventral than the lateral temporal cortex, consistent with a hypothesis arguing for early common face processing in the ventral temporal cortex. Further, emotion decoding was possible from 60–150 Hz and below 30 Hz, and better and faster in the ventral than the lateral temporal cortex.</p>
</sec><sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Behavioral results</title>
<p>We recorded electrophysiological responses from nine neurosurgical subjects by showing them either face or checkerboard stimuli and having them perform a three-alternative forced choice task on the stimuli by pressing a button (<xref ref-type="fig" rid="pone-0003892-g001">Figure 1a</xref>). In a given session of 200 trials, the three alternatives were either [1, happy; 2, other; 3, fear] or [1, woman; 2, other; 3, man]. We call a session with the former alternatives an “emotion discrimination session” and that with the latter a “gender discrimination session”. In any session, we randomly interleaved checkerboard stimuli in 40 trials. In these control trials, we instructed subjects to choose the alternative “other” to indicate the stimulus was not a face. For the rest of the 160 trials, we presented either a male or a female face (80 trials each) whose expression was neutral. After 1 second, we morphed the facial expression into either happy or fear over 0.5 seconds (40 trials each for male/happy, male/fear, female/happy, and female/fear). For those trials, we instructed subjects to choose the alternative that best described the stimulus. Behavioral responses were obtained in 19 of 22 sessions (3800 trials in total). The emotion and gender discrimination accuracy was 86.7±5.8% and 95.3±2.0% correct, respectively (mean+−s.e.m.), which were not significantly different (p&gt;0.5).</p>
</sec><sec id="s2b">
<title>ERP and spectrogram analysis</title>
<p>We recorded field potentials from subdural electrodes that covered the ventral and the lateral temporal cortex (<xref ref-type="fig" rid="pone-0003892-g001">Fig. 1B–E</xref>). Before proceeding into our novel decoding approaches, we first describe an example of the results from more conventional ERP and spectrogram analyses. In <xref ref-type="supplementary-material" rid="pone.0003892.s001">Figure S1</xref> and <xref ref-type="supplementary-material" rid="pone.0003892.s002">S2</xref>, we show the results from two responsive electrodes in the ventral temporal cortex (data from the first session for subject 153; for the locations of electrode 74 and 75, see <xref ref-type="supplementary-material" rid="pone.0003892.s003">Figure S3</xref>).</p>
<p>First, by averaging the raw field potentials in the time domain, we carried out an event-related potential (ERP) analysis (<xref ref-type="supplementary-material" rid="pone.0003892.s001">Figure S1A</xref> and <xref ref-type="supplementary-material" rid="pone.0003892.s002">S2A</xref>). At the abrupt onset of the static stimuli (t = 0 sec), field potentials were evoked in a stimulus-locked manner, resulting in larger positive or negative deflections to faces than to checkerboards at around 150–200 msec in many electrodes <xref ref-type="bibr" rid="pone.0003892-Allison1">[1]</xref>–<xref ref-type="bibr" rid="pone.0003892-Halgren1">[4]</xref>. For the exemplar electrodes, t-scores from two-tailed t-tests (comparing face&gt;checkerboard) exceeded t&gt;6 (<xref ref-type="supplementary-material" rid="pone.0003892.s001">Figure S1B</xref>, red), t&lt;−12 and t&gt;14 (<xref ref-type="supplementary-material" rid="pone.0003892.s002">Figure S2B</xref>, red) around 200 msec from the onset of the static stimuli (uncorrected for multiple comparisons). In contrast, we rarely found such a clear ERP during the time interval of our dynamic emotion morph period (t = 1–1.5 sec). Most likely, because the stimuli were morphed smoothly over 500 msec, field potentials were not locked to the onset of the morph. As a result, we found very few ERPs (<xref ref-type="supplementary-material" rid="pone.0003892.s001">Figure S1A</xref> and <xref ref-type="supplementary-material" rid="pone.0003892.s002">S2A</xref>) that discriminated dynamic facial morphs from dynamic checkerboard movies (<xref ref-type="supplementary-material" rid="pone.0003892.s001">Figure S1B</xref> and <xref ref-type="supplementary-material" rid="pone.0003892.s002">S2B</xref>, red). In particular, we almost never found any strong ERP that discriminated fearful from happy expressions during the morph period (<xref ref-type="supplementary-material" rid="pone.0003892.s001">Figure S1B</xref> and <xref ref-type="supplementary-material" rid="pone.0003892.s002">S2B</xref>, blue).</p>
<p>Second, we estimated the event-related power spectrogram of the ECoG for each trial using a multi-taper spectral analysis <xref ref-type="bibr" rid="pone.0003892-Mitra1">[5]</xref>, <xref ref-type="bibr" rid="pone.0003892-Pesaran1">[6]</xref> and obtained the average of the spectrograms for each condition (<xref ref-type="supplementary-material" rid="pone.0003892.s001">Figure S1C–E</xref> and <xref ref-type="supplementary-material" rid="pone.0003892.s002">S2C–E</xref>). The multi-taper method involves the use of multiple data tapers (i.e., the prolate spheroidal Slepian functions) for spectral estimation, which stabilizes the estimate of the power spectrum over short segments of possibly non-stationary data, suited for an analysis of intracranial EEG. The estimated spectrograms showed well-known 1/f power distributions. In addition, for the exemplar electrode, the spectrograms for fearful and happy faces showed stronger evoked responses around 100 Hz just after the onset of the morph period (t = 1–2 sec), which was absent for checkerboards. T-scores from two-tailed t-tests revealed significantly larger responses to dynamic faces than moving checkerboards (uncorrected for multiple comparisons; 50–200 Hz, t = 1–2.5 sec, <xref ref-type="supplementary-material" rid="pone.0003892.s001">Figure S1F</xref> and <xref ref-type="supplementary-material" rid="pone.0003892.s002">S2F</xref>), and in particular, to fear morphs than to happy morphs (50–150 Hz, t = 1.2–2, <xref ref-type="supplementary-material" rid="pone.0003892.s001">Figure S1G</xref>). By limiting our frequency of interest to 50–150 Hz (high-gamma band) we found that the high-gamma response was enhanced by the appearance of static faces, but not by static full-contrast checkerboard patterns (t = 0.1–1 sec), and that it was disproportionately enhanced by fearful (<xref ref-type="supplementary-material" rid="pone.0003892.s001">Figure S1H</xref>, blue) rather than happy morphs (<xref ref-type="supplementary-material" rid="pone.0003892.s001">Figure S1H</xref>, red; recorded in channel 75 during the window 1.2&lt;t&lt;2 sec).</p>
<p>While the above analysis approach is commonly used in many EEG studies, it posed problems for our data. By applying t-tests at each time-frequency point, we faced massive multiple comparison problems. Even worse, we analyzed ∼100 electrodes and picked one of the best electrodes for <xref ref-type="supplementary-material" rid="pone.0003892.s001">Figures S1</xref> and <xref ref-type="supplementary-material" rid="pone.0003892.s002">S2</xref>, further raising a concern for multiple comparisons. Strict correction, such as Bonferroni correction that assumes independent multiple hypothesis testing, would be unnecessarily strict because we often see strong correlation in signals across time, frequencies and neighboring electrodes. In the above approach, we defined the frequency of interest post-hoc; strictly speaking, our choice of frequency bands cannot be justified without prior independent studies. In practice, the best frequency bands were different from electrode to electrode, and from subject to subject. The best frequency bands also often depended on the testing condition. Thus, prior specification of a frequency band of interest could lead to poor statistical power for detecting any real positive effects. Or, if it is specified to maximize the effect in a particular study, it may over-fit the data and generalize poorly.</p>
<p>In order to address these problems, we applied multi-variate decoding analyses, which optimally linearly combined signals. Our decoding approaches objectively reduced the dimensionality of the signal, alleviating multiple comparison problems. A trained linear classifier learned correlations across time, frequency and electrodes in an appropriate way to optimize the decoding performance. By training the classifier with regularization <xref ref-type="bibr" rid="pone.0003892-Poggio1">[7]</xref> and validating the classifier against an untrained data set, we were able to retain high sensitivity with much less over-fitting. In the following, we describe our decoding approaches and the results obtained from the decoding analyses.</p>
</sec><sec id="s2c">
<title>Decoding analyses</title>
<p>As inputs to the linear classifier, we used the logarithm of the power estimated via the multi-taper method from each trial. We trained a regularized least-square classifier <xref ref-type="bibr" rid="pone.0003892-Poggio1">[7]</xref>, <xref ref-type="bibr" rid="pone.0003892-Hung1">[8]</xref> on randomly chosen 70% of the trials and tested its decoding performance on the remaining 30% of the trials for each session in each subject. We evaluated the decoding accuracy by submitting the classifier outputs into the receiver operating characteristic (ROC) analysis <xref ref-type="bibr" rid="pone.0003892-Macmillan1">[9]</xref>, rather than assigning a binary correct or incorrect label for each trial and computing % of correct classification. ROC analysis allowed us to utilize the information present in the magnitude of output from the classifier (i.e., an output close to 0 when inputs cannot be confidently classified as X or Y and an output far from 0 when inputs for a test trial is easily classified as X or Y). We submitted the graded classifier outputs for all test trials into the ROC analysis and computed the area under the ROC curve (Throughout the paper, we call the area under ROC curve A' for short).</p>
<p>Here we introduce three novel decoding approaches: 1) time-frequency decoding map, 2) time course of decoding, and 3) searchlight decoding. The time-frequency decoding map was obtained for each session in each subject by combining information across electrodes within a certain anatomical region at each time step at each frequency band with linear weights, taking into account the spatial correlation across electrodes. This map emphasizes the most informative time-frequency points, reducing the space dimension in an optimally linear manner. The time course of decoding for each session was obtained by combining information across electrodes (space) and frequencies, reducing the space and frequency dimensions in an optimally linear manner (<xref ref-type="fig" rid="pone-0003892-g002">Figure 2</xref>). The time course analysis provides the latency for decoding, an earliest estimate of the time when the information becomes available in a circumscribed anatomical region. Searchlight decoding combines the signals from a small cluster of contiguous electrodes, and scans throughout the cortical surface covered by all electrodes. Thus, the resulting searchlight decoding map retains spatial information. This allowed us to map electrode locations on the brain surface according to the maximal amount of information that they might carry, comparable to similar approaches used in functional neuroimaging (Kriegeskorte and Bandettini, 2007).</p>
<fig id="pone-0003892-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0003892.g002</object-id><label>Figure 2</label><caption>
<title>Decoding procedure.</title>
<p>The different parts of the figure provide a schematic, using real data as an example, of how power spectrograms estimated in each electrode (the colored spectrograms in the middle) can be pooled to decode stimulus category (fear vs. happy in this example). (Middle) The average event-related spectrogram (colored graphs) was obtained for each electrode (“Chan1”…“ChanN”) under two different conditions (in this example, happy and fearful trials). Note that neighboring electrodes can show highly variable and complex responses at different frequencies (color-coded from −2 to +2 dB in channel 1, from −2 to +5 dB in channel 2, and from −3.5 to 1.5 dB in channel N). In the example depicted in the figure we show only 3 channels out of a typically much larger number, but the problem of visualization and statistical analysis is already apparent. (Top) Time-frequency decoding map for the ventral temporal cortex of one subject. Color code represents area under the ROC curve (A'). For example, the red pixel at 1.7 sec and 70 Hz (black arrow) means that when we combine the power at that time-frequency point from all the electrodes in the ventral temporal cortex with a linear weight (estimated from 70% of the training trials), the classifier can discriminate happy from fear with A' = .60 (or 60% correct classification with an arbitrary criterion) for the test trials. (Bottom) In order to characterize the latency of decoding, we combined the power across frequencies and electrodes. The peak decoding accuracy is A' = .64 for the bottom panel while it is A' = .60 in the top panel, showing an advantage in combining information across frequencies in addition to across electrodes. Decoding across frequencies also facilitates comparison across subjects because the peak of the sensitive frequency bands can vary across subjects but remain relatively constant in time across subjects.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0003892.g002" xlink:type="simple"/></fig>
<p>Based on their location, we grouped electrodes as belonging to either the ventral or the lateral temporal cortex (<xref ref-type="fig" rid="pone-0003892-g001">Figure 1B and C</xref>). As is typical in field potential recordings during epilepsy monitoring, precise electrode locations varied across subjects, making comparisons across subjects difficult with a conventional analysis. Our decoding analyses are powerful alternative ways to solve this problem, since they optimally blur the precise anatomical location of electrodes, which is variable from subject to subject.</p>
</sec><sec id="s2d">
<title>Temporal characteristics of face processing</title>
<p>First, we combined the logarithm of the event-related power from all electrodes, separately for the ventral and the lateral temporal cortex, and computed the time-frequency decoding map to characterize the critical time-frequency points for face processing. For face vs. checkerboard discrimination, decoding performance in the ventral temporal cortex was very high, with most information contained in a frequency band of 50–150 Hz (<xref ref-type="fig" rid="pone-0003892-g003">Figure 3A</xref>) shortly after the stimulus onset (0.1–0.5 sec had A' = 0.84–0.86) as well as after the onset of morphing epoch (1–2 sec had A' = 0.75–0.80). Decoding performance in the lateral temporal cortex was lower, and while most information was similarly contained in a band of 50–150 Hz after the onset of the static stimulus (A' = 0.65), it was represented more broadband (0–150 Hz) after the onset of the morphing epoch (A' = 0.63–0.67; <xref ref-type="fig" rid="pone-0003892-g003">Figure 3B</xref>).</p>
<fig id="pone-0003892-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0003892.g003</object-id><label>Figure 3</label><caption>
<title>Decoding performance for face vs. checkerboard discrimination in the ventral (A and C) and the lateral (B and D) temporal cortex.</title>
<p>A and B Time-frequency decoding map. The onset of static (<italic>t</italic> = 0) and morph (<italic>t</italic> = 1) and the offset of morph (<italic>t</italic> = 1.5) and static (<italic>t</italic> = 2.5) stimuli are indicated by vertical lines. Two peaks of decoding performance are found centered around 100 Hz. Only significant pixels are color-coded (FDR q&lt;0.1; p&lt;0.063 for A and p&lt;0.054 for B). C and D Time course of decoding, combining the power across electrodes and frequencies. We marked with circles the time points where the decoding performance is significantly above chance (FDR q&lt;0.05; p&lt;0.035 for C and p&lt;0.011 for D). One standard error of the mean is shown by blue shading. For this analysis we used a 100-msec window with a step size of 50 msec, with an effective frequency resolution of 20 Hz. For the ventral temporal cortex (A and C), we obtained the data from 21 sessions in 8 subjects and pooled across 23.8 electrodes (mean across sessions). For the lateral temporal cortex (B and D), we obtained the data from 22 sessions in 9 subjects and pooled across 79.1 electrodes.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0003892.g003" xlink:type="simple"/></fig>
<p>By combining all frequencies across electrodes, we further characterized the decoding time course to obtain decoding latency and maximum decoding accuracy (<xref ref-type="fig" rid="pone-0003892-g003">Figure 3C and D</xref>, <xref ref-type="table" rid="pone-0003892-t001">Table 1</xref>). The ventral temporal cortex showed higher decoding accuracy than the lateral temporal cortex throughout the stimulus presentation, including the morphing epoch. The emotional facial movement evoked activity related to discrimination of faces from checkerboard in both the ventral and the lateral temporal cortices, with the former carrying more information than the latter.</p>
<table-wrap id="pone-0003892-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0003892.t001</object-id><label>Table 1</label><caption>
<title>Summary of decoding performance.</title>
</caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pone-0003892-t001-1" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0003892.t001" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" colspan="1" rowspan="1"/>
<td align="left" colspan="1" rowspan="1">Patients</td>
<td align="left" colspan="1" rowspan="1">Sessions</td>
<td align="left" colspan="1" rowspan="1">Electrodes</td>
<td align="left" colspan="1" rowspan="1">Static Face vs. Checker</td>
<td align="left" colspan="1" rowspan="1">Dynamic Face vs. Checker</td>
<td align="left" colspan="1" rowspan="1">Dynamic Happy vs Fear</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">Ventral temporal cortex</td>
<td align="left" colspan="1" rowspan="1">8</td>
<td align="left" colspan="1" rowspan="1">21</td>
<td align="left" colspan="1" rowspan="1">23.8±16.4</td>
<td align="left" colspan="1" rowspan="1">A' = 0.86 (87 msec)</td>
<td align="left" colspan="1" rowspan="1">A' = 0.80</td>
<td align="left" colspan="1" rowspan="1">A' = 0.61 (344 msec)</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">Lateral temporal cortex</td>
<td align="left" colspan="1" rowspan="1">9</td>
<td align="left" colspan="1" rowspan="1">22</td>
<td align="left" colspan="1" rowspan="1">79.0±15.5</td>
<td align="left" colspan="1" rowspan="1">A' = 0.65 (240 msec)</td>
<td align="left" colspan="1" rowspan="1">A' = 0.67</td>
<td align="left" colspan="1" rowspan="1">A' = 0.55 (946 msec)</td>
</tr>
</tbody>
</table></alternatives><table-wrap-foot><fn id="nt101"><p>Peak decoding performance (in A') and the latency of the decoding, defined as the first time point when the decoding became significantly above chance (FDR q&gt;0.05).</p></fn></table-wrap-foot></table-wrap>
<p>How much of this discriminatory information was coming from the response to faces, rather than to checkerboards? Assuming that high-gamma power was correlated with local multi-unit activity <xref ref-type="bibr" rid="pone.0003892-Logothetis1">[10]</xref>–<xref ref-type="bibr" rid="pone.0003892-Nir2">[12]</xref>, we examined whether the average high-gamma power (50–150 Hz, from t = 1.1–1.9 sec, during the morph period) was higher for faces than for checkerboards. For the ventral temporal cortex, compared to checkerboards, the high-gamma responses to faces were higher (t-score&gt;3 from paired t-test, uncorrected for multiple comparisons) in 10.4% of electrodes ( = 33/316) and lower (t-score&lt;−3) in 3.5% of electrodes ( = 11/316). For the lateral temporal cortex, the pattern was opposite; the high-gamma responses for faces were higher in 3.3% and lower in 6.7% of electrodes. The entire distribution of t-scores was significantly more positive (i.e, more electrodes showed higher responses to faces than to checkerboards) in the ventral than the lateral temporal cortex (p&lt;1e-8, Kolmogorov-Smirnov test). While we do not claim that decoding accuracy was solely dependent on the specific high-gamma increase to faces, we conclude that decoding accuracy in the ventral temporal cortex was heavily dependent on the increased power evoked by faces.</p>
</sec><sec id="s2e">
<title>Ventral temporal cortex discriminates emotion more rapidly and accurately than lateral temporal cortex</title>
<p>A dominant view of face perception proposes that regions in dorsal and lateral temporal cortex, notably the area around the STS, are specialized for processing changeable facial features that are important for social communication, including facial expressions <xref ref-type="bibr" rid="pone.0003892-Haxby1">[13]</xref>, <xref ref-type="bibr" rid="pone.0003892-Allison2">[14]</xref>. Contrary to this view, we found that emotion decoding performance was better and faster in the ventral than in the lateral temporal cortex (<xref ref-type="fig" rid="pone-0003892-g004">Figure 4</xref>).</p>
<fig id="pone-0003892-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0003892.g004</object-id><label>Figure 4</label><caption>
<title>Ventral temporal cortex discriminates emotional expression more quickly and accurately than lateral temporal cortex.</title>
<p>A–C Time course of decoding. D and E Time-frequency decoding map. A, B, D, E The results for the ventral temporal cortex and C for the lateral temporal cortex. For A and D, we used a 100-msec time window with a step size of 100 msec, giving an effective frequency resolution of 20 Hz. For B, C, and E, we used a 500-msec window with a step size of 100 msec, giving an effective frequency resolution of 4 Hz.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0003892.g004" xlink:type="simple"/></fig>
<p>Using a time window of 100 msec, the time-frequency decoding map for emotion discrimination reached above chance only for the ventral temporal cortex. The decoding accuracy for the ventral temporal cortex showed two peaks: one in the high gamma range and the other at lower frequencies (.3 sec to .9 sec after morph onset; FDR q&lt;0.1, p&lt;0.0091; <xref ref-type="fig" rid="pone-0003892-g004">Figure 4D</xref>). To resolve the two peaks in lower frequencies, we used a longer time window of 500 msec and an effective frequency resolution of 4 Hz. For this analysis, we used a step size of 100 msec and analyzed the data up to 100 Hz. With this resolution, we found one peak at the frequency below 30 Hz and the other peak above 60 Hz (q&lt;0.1, p&lt;0.016) (<xref ref-type="fig" rid="pone-0003892-g004">Figure 4E</xref>). The lateral temporal cortex did not show a consistent time-frequency decoding map across subjects and none of the time-frequency pixels survived the statistical threshold (FDR q&gt;0.1).</p>
<p>We analyzed the time course of decoding using a short (100 msec) and a long (500 msec) time window. With the shorter time window and smaller time steps, emotion decoding performance reached above chance only in the ventral temporal cortex (peak A' = 0.57, 0.34 sec after morph onset, q&lt;0.05, p&lt;0.0093). With the longer window and better frequency resolution, emotion decoding accuracy reached above chance in both regions, however, it was better and faster in the ventral (peak A' = 0.61, 0.34 sec after the morph onset, q&lt;0.05, p&lt;0.012) than in the lateral temporal cortex (peak A' = 0.55, 0.95 sec after the morph onset, q&lt;0.05, p&lt;0.0066).</p>
<p>Taken together, our results are consistent with the hypothesis that the ventral temporal cortex performs an initial analysis of several aspects of faces, which would include diagnostic information about the categorization of the facial expression <xref ref-type="bibr" rid="pone.0003892-Schyns1">[15]</xref>, <xref ref-type="bibr" rid="pone.0003892-Smith1">[16]</xref>, whereas the lateral temporal cortex appears to be more important for later stages of processing, possibly related to integration of the information across different modalities and to motor planning for social interaction <xref ref-type="bibr" rid="pone.0003892-Calder1">[17]</xref> (see <xref ref-type="sec" rid="s3">Discussion</xref> for further considerations).</p>
</sec><sec id="s2f">
<title>Task-relevant attention improves decoding performance across frequencies</title>
<p>Decoding analyses also provide insights into the effects of task-related attentional modulation. We manipulated subjects' attention with the task instruction. In the sessions where subjects performed the emotion discrimination task, we expected they would pay more attention to faces at the beginning of the morph period because emotional expression was first revealed at that point in time. On the other hand, in the sessions where they performed the gender discrimination task, we expected they would pay more attention at the beginning of the static period. In the ventral temporal cortex, decoding accuracy for discriminating faces from checkerboard (A'<sub>fc</sub>) was above chance both in the emotion- and in the gender- discrimination sessions (<xref ref-type="fig" rid="pone-0003892-g005">Figure 5A</xref>) (The results shown were obtained with a 500 msec time window, but similar results were also obtained with a 100 msec window, data not shown). A'<sub>fc</sub> during the morph period was significantly better in the emotion- (the peak A'<sub>fc</sub> = 0.85) than in the gender- discrimination sessions (the peak A'<sub>fc</sub> = 0.74); the difference (i.e., A'<sub>fc</sub> [in emotion sessions]−A'<sub>fc</sub> [in gender sessions]) reached significance right after the start of the morph period (<italic>t</italic> = 1.15 sec from the stimulus onset), attained its peak of 0.11 at <italic>t</italic> = 1.5 sec, and remained until the subject's button-push response (<italic>t</italic> = 2 sec) (<xref ref-type="fig" rid="pone-0003892-g005">Figure 5B</xref>, q&lt;0.05, p&lt;0.02). To examine which frequency bands are responsible for these attentional effects, we used time-frequency decoding maps and computed their A'<sub>fc</sub> difference. Interestingly, the attentional effects were not localized in particular frequencies, but distributed across frequencies (<xref ref-type="fig" rid="pone-0003892-g005">Figure 5C</xref>, q&lt;0.1, p&lt;0.0079).</p>
<fig id="pone-0003892-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0003892.g005</object-id><label>Figure 5</label><caption>
<title>Task-relevant attention improves decoding.</title>
<p>A Decoding performance for discriminating face vs. checkerboard (A'<sub>fc</sub>) when subjects were performing the emotion- (blue) or the gender- (green) task in the ventral temporal cortex. Circles mark the points where the decoding performance is significantly above chance (q&lt;0.05, p&lt;0.029). We combined 24.0 electrodes across all frequency bands. B The mean difference in decoding accuracy for A'<sub>fc</sub> in the emotion-task minus A'<sub>fc</sub> in the gender-task sessions (thick red line). The positive difference reached significance, marked by circles around the time when the stimuli started to morph (<italic>t</italic> = 1; q&lt;0.05, p&lt;0.012). The peak difference reached 11% at the maximum. Shading represents one standard error above and below the mean. C Time-frequency map for the difference in A'<sub>fc</sub>. The attentional effects were distributed across the frequencies. The map is thresholded at FDR q&lt;0.1, p&lt;0.0079. For this analysis, we used a time window of 500 msec with a step size of 100 msec, the effective frequency resolution was 4 Hz.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0003892.g005" xlink:type="simple"/></fig>
<p>We further hypothesized that the decoding accuracy for discriminating emotion (A'<sub>em</sub>) and gender (A'<sub>gn</sub>) would be modulated by the task-instruction; A'<sub>em</sub> should be higher in the emotion- than that in the gender- discrimination sessions, while A'<sub>gn</sub> should be higher in the gender- than that in the emotion- discrimination sessions. However, we did not observe any such effects (See <xref ref-type="sec" rid="s3">Discussion</xref>.) For the lateral temporal cortex, we did not find any significant attentional effects of any kind.</p>
</sec><sec id="s2g">
<title>Searchlight decoding for anatomical information</title>
<p>To reveal the anatomical organization of face processing, we created an electrode-based decoding map. In conventional analyses, response selectivity of each electrode is mapped by averaging the power with some cutoff for the frequency band (such as below or above 50 Hz in <xref ref-type="fig" rid="pone-0003892-g006">Figure 6A and B</xref>). In our single electrode-based decoding analysis, we combined the power in each electrode optimally linearly across frequency and time (100–900 msec from the stimulus onset, <xref ref-type="fig" rid="pone-0003892-g006">Figure 6C</xref>). Combination of several neighboring electrodes within a “searchlight” improved the decoding accuracy (<xref ref-type="fig" rid="pone-0003892-g006">Figure 6D</xref>). Here, a “searchlight” is defined as a narrow circular field of view, which contains on average four neighboring electrode contacts. To create a searchlight decoding map, we scanned through the cortex by the searchlight to cover all the electrodes.</p>
<fig id="pone-0003892-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0003892.g006</object-id><label>Figure 6</label><caption>
<title>Searchlight decoding map in the ventral temporal cortex.</title>
<p>A–D Comparison of the classic analysis (A, average power below 50 Hz; B, above 50 Hz) and decoding analysis (C, single-electrode based decoding; D, searchlight decoding). A and B The mean event-related power from 100–900 msec from the onset of the morph epoch was contrasted between the happy and the fear morphing trials. The difference was evaluated with a t-test and thresholded at |t|&gt;1. Red color indicates greater response to happy, blue indicates greater response to fear. C Single-electrode based decoding. D Searchlight decoding for the same subject (subject 153, two sessions). Here, the spectrogram was estimated with a time window of 500 msec, with a step size of 100 msec as in <xref ref-type="fig" rid="pone-0003892-g005">Figure 5A and B</xref>. E The average decoding map across 8 subjects (see <xref ref-type="sec" rid="s4">Method</xref> for how we averaged decoding maps). Decoding accuracy in the bilateral middle ventral temporal cortex improved substantially by pooling neighboring electrodes (top), compared to the near-chance decoding obtained with a single-electrode decoding analysis (bottom). For emotion decoding (middle), the pooling by searchlight improves the decoding performance substantially. For the gender decoding (right two columns), there are discriminating clusters in the middle fusiform gyri. Note the best decoding accuracy originates roughly from the same locations among five panels in the top row. Each color-coded pixel shown was covered by electrodes from at least two subjects.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0003892.g006" xlink:type="simple"/></fig>
<p>To obtain a single-electrode-based decoding map across subjects, we smoothed the decoding accuracy and then averaged across subjects (See <xref ref-type="sec" rid="s4">Methods</xref>). The results for the ventral temporal cortex are shown in the bottom row of <xref ref-type="fig" rid="pone-0003892-g006">Figure 6E</xref>. The relatively poor decoding performance is expected for several reasons in addition to the general advantage in the searchlight decoding (<xref ref-type="fig" rid="pone-0003892-g006">Figure 6E, top row</xref>). First, sensitive electrodes with high decoding accuracy are often abutted by poor neighboring electrodes as can be seen in <xref ref-type="fig" rid="pone-0003892-g006">Figure 6C</xref>. Such a situation often arises when two electrodes are separated by a sulcus, reflecting an anatomical discontinuity. In this case, simple spatial smoothing degrades the performance of the best electrodes. Second, the precise locations of the best electrodes are not consistent across subjects, resulting in further deterioration of the apparent single-electrode decoding accuracy when decoding maps from multiple subjects are averaged.</p>
<p>Searchlight decoding solves these problems. This approach uses an optimal weighting among locally adjacent electrodes, so that the resulting map retains the anatomical information about electrode location while being more robust with respect to inter-subject variability; even if exact locations of the most sensitive electrode vary across subjects, the averaging on the searchlight decoding map does not destroy such a local peak information as in the case of the single electrode-based decoding explained above.</p>
<p>Searchlight decoding revealed information-carrying regions that correspond to the FFA in the ventral cortex (<xref ref-type="fig" rid="pone-0003892-g006">Figure 6E, top row</xref>) and to the STS in the lateral temporal cortex (data not shown). Discriminating faces from checkerboard pattern (A'<sub>fc</sub>) in the right middle fusiform gyri reached almost 100% (<xref ref-type="fig" rid="pone-0003892-g006">Figure 6E, left two columns in the top row</xref>). The apparent discrepancy between this perfect searchlight decoding and the maximal decoding performance in the time course of decoding (A'<sub>fc</sub> = .85, <xref ref-type="fig" rid="pone-0003892-g002">Figure 2C</xref>) is due to sampling bias of the electrodes and the subjects in the searchlight decoding. As can be seen in <xref ref-type="fig" rid="pone-0003892-g001">Figure 1B</xref>, four subjects had electrodes roughly around this right middle fusiform region. To see if this right middle fusiform region always contains the most sensitive electrode, further studies would be needed. It is interesting to note that the right FFA-like region seems to carry information about emotion (<xref ref-type="fig" rid="pone-0003892-g006">Figure 6E, middle</xref>) and gender (<xref ref-type="fig" rid="pone-0003892-g006">Figure 6E, right two columns</xref>). To perform the appropriate statistics here, however, we would need a larger sample of subjects.</p>
<p>Though it is difficult to perform statistical analysis on the searchlight decoding map, we see a hint of hemispheric specialization in terms of decoding accuracy in <xref ref-type="fig" rid="pone-0003892-g006">Figure 6E</xref>. We followed up these possible laterality effects in a final analysis.</p>
</sec><sec id="s2h">
<title>Laterality effects</title>
<p>Using searchlight decoding, we had observed an apparent right hemisphere dominance for face processing, consistent with prior studies <xref ref-type="bibr" rid="pone.0003892-DeRenzi1">[18]</xref>–<xref ref-type="bibr" rid="pone.0003892-Fried1">[21]</xref>. Here, we grouped electrodes in each hemisphere into three subregions; the anterior and the posterior lateral temporal cortices, and the ventral temporal cortex. Decoding was performed by combining across time and frequency in the same way as searchlight decoding.</p>
<p>We found evidence for a right hemispheric dominance in the anterior STS (<xref ref-type="fig" rid="pone-0003892-g007">Figure 7A</xref>) and the ventral temporal cortex (<xref ref-type="fig" rid="pone-0003892-g007">Figure 7B</xref>). In the anterior STS, decoding accuracy for discrimination of face vs checkerboard (A'<sub>fc</sub>) and emotion (A'<sub>em</sub>) was above chance in the right hemisphere (p&lt;0.01 for A'<sub>fc</sub>, and p&lt;0.001 for A'<sub>em</sub>, t-test with a null hypothesis of A' = 0.5; 11 sessions, 5 subjects, mean number of electrodes = 40.0) but not in the left hemisphere (p&gt;0.05 for both A'<sub>fc</sub> and A'<sub>em</sub>; 11 sessions, 4 subjects, mean number of electrodes = 38.4). The difference was significant (two-tailed unpaired t-test, p&lt;0.05 and p&lt;0.01 for A'<sub>fc</sub> and A'<sub>em</sub>, respectively). In the ventral temporal cortex, decoding accuracy for discrimination of gender (A'<sub>gn</sub>) was above chance in the right (p&lt;0.01, 17 sessions, 7 subjects, mean number of electrodes = 18.9) but not in the left hemisphere (p&gt;0.05, 21 sessions, 8 subjects, mean number of electrodes = 11.0) with a significant difference (two-tailed unpaired t-test, p&lt;0.05). Unexpectedly, in the ventral temporal cortex, emotion decoding (A'<sub>em</sub>) was above chance in the left (p&lt;0.001) but not in the right hemisphere (p&gt;0.05), with a significant difference (p&lt;0.01). Though a right hemisphere advantage for face processing has been reported previously <xref ref-type="bibr" rid="pone.0003892-DeRenzi1">[18]</xref>–<xref ref-type="bibr" rid="pone.0003892-Fried1">[21]</xref>, the superior emotion processing in the left ventral temporal cortex has not (although there are reports of a left amygdala advantage <xref ref-type="bibr" rid="pone.0003892-Morris1">[22]</xref>). Decoding analysis, however, can reveal only the information available in principle, not how and whether that information is used by the brain to guide behavioral discrimination. It is possible that diagnostic facial features that are critical for emotion detection <xref ref-type="bibr" rid="pone.0003892-Smith1">[16]</xref> are processed automatically and represented more accurately in the left hemisphere, but that the integration of information required for behavioral discrimination is performed in the right hemisphere.</p>
<fig id="pone-0003892-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0003892.g007</object-id><label>Figure 7</label><caption>
<title>Laterality effects.</title>
<p>A In the anterior STS region, the decoding accuracy for discrimination of face from checkerboard (A'<sub>fc</sub>, left) and for discriminating emotion (A'<sub>em</sub>, right) was better in the right than that in the left hemisphere. B In the ventral temporal cortex, a right hemisphere dominance was also found for gender decoding (A'<sub>gn</sub>, right). However, the left hemisphere was superior to the right hemisphere when decoding emotion (A'<sub>em</sub>, left). *, **, and *** indicates the significance level of p&lt;0.05, p&lt;0.01, and p&lt;0.001, respectively.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0003892.g007" xlink:type="simple"/></fig></sec></sec><sec id="s3">
<title>Discussion</title>
<p>Considerable effort has been devoted to showing that, among other stimulus categories, faces are processed preferentially by specific anatomical structures <xref ref-type="bibr" rid="pone.0003892-Sergent1">[23]</xref>–<xref ref-type="bibr" rid="pone.0003892-Sato1">[26]</xref> and can evoke scalp EEG and MEG responses at certain latencies <xref ref-type="bibr" rid="pone.0003892-Schyns1">[15]</xref>, <xref ref-type="bibr" rid="pone.0003892-Bentin1">[27]</xref>–<xref ref-type="bibr" rid="pone.0003892-Halgren2">[33]</xref>. Yet anatomical space and detailed processing time have generally not been mapped jointly, in large part because doing so requires rarely available methods such as the intracranial recordings we present here <xref ref-type="bibr" rid="pone.0003892-Allison1">[1]</xref>–<xref ref-type="bibr" rid="pone.0003892-Halgren1">[4]</xref>, <xref ref-type="bibr" rid="pone.0003892-Seeck1">[28]</xref>, <xref ref-type="bibr" rid="pone.0003892-KrolakSalmon1">[34]</xref>, <xref ref-type="bibr" rid="pone.0003892-Lachaux1">[35]</xref>. We analyzed the intracranial ECoG with a decoding technique and found that 1) the best discrimination of faces from checkerboards arose within a critical frequency band of 50–150 Hz in the ventral temporal cortex, 2) this held for both static and dynamic stimuli, 3) the accuracy of decoding was much better in ventral as compared to lateral temporal cortex, for faces vs. checkerboards, and also for happiness vs. fear, 4) in the ventral temporal cortex, task-relevant attention improved the decoding accuracy for stimulus category (A'<sub>fc</sub>) across wide frequencies by as much as 11%, but it did not improve decoding accuracy for emotion (A'<sub>em</sub>) and gender (A'<sub>gn</sub>), and 5) the anterior STS and the ventral temporal cortex showed evidence for hemispheric specialization of face processing.</p>
<sec id="s3a">
<title>Role of ventral and lateral temporal cortex in face processing</title>
<p>An influential model of face processing <xref ref-type="bibr" rid="pone.0003892-Bruce1">[36]</xref> hypothesized that face identity and emotional expression are processed by functionally separate systems. More recently, this idea has been resurrected on the basis of findings from cognitive neuroscience: invariant features of faces (i.e., identity) appear to be processed predominantly in ventral temporal cortex, including the fusiform face area (FFA) <xref ref-type="bibr" rid="pone.0003892-Kanwisher1">[24]</xref>, <xref ref-type="bibr" rid="pone.0003892-Kanwisher2">[37]</xref>, while changeable features of faces, such as emotional expression, appear to be processed in lateral temporal cortex, including the superior temporal sulcus (STS) <xref ref-type="bibr" rid="pone.0003892-Haxby1">[13]</xref>, <xref ref-type="bibr" rid="pone.0003892-Allison2">[14]</xref>. This popular theory of face processing has been supported by neuropsychological studies of subjects with focal lesions, as well as by fMRI and single cell physiology (For reviews, see <xref ref-type="bibr" rid="pone.0003892-Haxby1">[13]</xref>, <xref ref-type="bibr" rid="pone.0003892-Bruce1">[36]</xref>). However, a careful review of the literature casts some doubt on the extent to which processing of face identity and expression are truly independent. For instance, while there are prosopagnosic subjects with severely impaired identity recognition yet spared emotion recognition, there is no known case of severely impaired general recognition of emotions with spared recognition identity (although there are cases with selective impairments in recognizing certain specific emotions <xref ref-type="bibr" rid="pone.0003892-Adolphs2">[38]</xref>, <xref ref-type="bibr" rid="pone.0003892-Phillips1">[39]</xref>). Whether identity and emotion information are processed by entirely separate neural structures is still open to debate <xref ref-type="bibr" rid="pone.0003892-Calder1">[17]</xref>.</p>
<p>We found that decoding performance in the ventral temporal cortex around the fusiform gyrus was much superior to the lateral temporal cortex, including the STS. The ventral superiority was expected during the static period; surprisingly, however, this held true for discrimination of faces from checkerboards during the morph period as well (A'<sub>fc</sub> at t = 1–2 sec). In our paradigm, the identity of a face is revealed to subjects at the onset of a trial and remains constant throughout the trial, in particular, it is constant during the morph period. Thus, no additional information relevant to face vs. checkerboard discrimination is revealed during the morph period. Decoding performance A'<sub>fc</sub> in the ventral temporal cortex peaked immediately after the stimulus onset (A'<sub>fc</sub> = 0.85), and after it fell below A'<sub>fc</sub>&lt;0.6, it quickly improved at the onset of morphing (A'<sub>fc</sub> = 0.77, <xref ref-type="fig" rid="pone-0003892-g003">Figure 3</xref>). Our analysis based on the average high-gamma activity during the morph period showed that many electrodes in the ventral temporal cortex increased activity to faces, while those in the lateral temporal cortex increased activity to checkerboard patterns. This pattern of results strengthens the idea that the ventral temporal cortex serves as a general ‘face processor’, which responds to facial movements, even without any change in identity. While we cannot rule out the possibility that motion of the stimulus strongly attracted attention and therefore activated the ventral temporal cortex, we note that such facial motion would be expected to attract more attention in the lateral than the ventral temporal cortex according to the standard view. Our results were not consistent with what that theory would predict. Further studies will be needed to investigate whether the same ventral temporal region also responds to other types of biological motion, such as gaze shifts or movements of the mouth during speech.</p>
<p>Better and faster emotion decoding accuracy (A'<sub>em</sub>) in the ventral than the lateral temporal cortex indicates that information processed within the ventral cortex at early latencies might contain critical and sufficient information to discriminate emotional expressions. It is plausible that the activation in the ventral temporal cortex reflects automatic processing of some facial features such as eyes <xref ref-type="bibr" rid="pone.0003892-Schyns1">[15]</xref>, diagnostic for certain emotion categorizations <xref ref-type="bibr" rid="pone.0003892-Smith1">[16]</xref>, but may not be causally related to the recognition of emotional expression, which can be supported by regions other than the ventral temporal cortex (Adolphs, 2002).</p>
<p>The worse decoding accuracy for emotion (A'<sub>em</sub>) in the lateral than the ventral temporal cortex might be due to other reasons. It is possible that the collective neuronal activity measured by our field potential recordings may have smeared out more fine-grained encoding of emotional expression information at the level of single neurons <xref ref-type="bibr" rid="pone.0003892-Sugase1">[40]</xref>, <xref ref-type="bibr" rid="pone.0003892-Young1">[41]</xref>, and that this effect may have differentially affected the regions around the putative FFA and STS. Such spatiotemporally fine-grained information would not have been detected by individual surface electrodes because the detectable information reflects the integrated activity at the level of a neuronal population. Another potential reason may be that our surface electrodes were less sensitive to information from cortex buried within sulcal folds, as they are for information from subcortical structures with non-uniform dendritic arborization <xref ref-type="bibr" rid="pone.0003892-Mitzdorf1">[42]</xref>. If an electrode were buried in the sulcus of the lateral temporal cortex, emotion decoding performance might improve substantially, and possibly better than for the ventral temporal cortex.</p>
</sec><sec id="s3b">
<title>Rapid categorization in ventral temporal cortex?</title>
<p>We found that field potentials recorded from the ventral temporal cortex discriminate faces from checkerboards rapidly. Previous EEG <xref ref-type="bibr" rid="pone.0003892-Seeck1">[28]</xref>–<xref ref-type="bibr" rid="pone.0003892-Pizzagalli1">[31]</xref>, <xref ref-type="bibr" rid="pone.0003892-VanRullen1">[43]</xref>, <xref ref-type="bibr" rid="pone.0003892-Foxe1">[44]</xref> and MEG studies <xref ref-type="bibr" rid="pone.0003892-Liu1">[32]</xref> found an early evoked potential before or around 100 msec that is correlated with stimulus categorization. This rapid categorization may not reflect subjects' decisions to categorize the stimulus, but rather statistical image properties of the different categories of the stimuli <xref ref-type="bibr" rid="pone.0003892-VanRullen1">[43]</xref>. Alternatively, this rapid response may be correlated with behavioral categorization, especially for categorization of stimuli as face vs. non-face objects <xref ref-type="bibr" rid="pone.0003892-Liu1">[32]</xref>. Because we were originally motivated to study the neuronal response during dynamically morphing facial expressions, our choice of control stimuli (high-contrast checkerboards) was not optimal to study rapid categorization of objects at such a short latency. We are now addressing these questions and extending our current findings by using other classes of stimuli. Can rapid response in the ventral cortex categorize several classes of objects?</p>
<p>We note that the exact relationship between the rapid power modulation in high-frequency bands that we observed and the early component reported in EEG and MEG previously is unclear. The early EEG/MEG component is dominated by the stimulus-locked (i.e., response phase is constant across trials) power modulation in low frequency bands with minimal contribution from high frequency bands because the power spectrum of the electroencephalogram has a 1/f distribution (<xref ref-type="supplementary-material" rid="pone.0003892.s001">Figures S1</xref> and <xref ref-type="supplementary-material" rid="pone.0003892.s002">S2</xref> C–E). Our decoding was mainly based on power modulation in high-frequency bands (e.g., <xref ref-type="fig" rid="pone-0003892-g003">Figure 3</xref>). An interaction between low and high frequency responses was demonstrated in a recent study <xref ref-type="bibr" rid="pone.0003892-Canolty1">[45]</xref> that found robust coupling between the phase of the theta rhythm (4–8 Hz) and the power of high gamma responses. Further studies using a stimulus-evoked response paradigm will be required to reveal the relationship between the transient power increase we found in high-frequency bands and its power modulation by the phase of lower frequency oscillations.</p>
</sec><sec id="s3c">
<title>Attentional improvement of decoding performance</title>
<p>Depending on the task instruction, the decoding accuracy for discriminating faces from checkerboards (A'<sub>fc</sub>) improved by 11% in the ventral temporal cortex. During the morph period, attention to facial emotion improved A'<sub>fc</sub> across wide frequencies compared to attention to facial gender (<xref ref-type="fig" rid="pone-0003892-g006">Figure 6C</xref>).</p>
<p>Task-relevant top-down attention is known to modulate neuronal firing rate <xref ref-type="bibr" rid="pone.0003892-Desimone1">[46]</xref>, <xref ref-type="bibr" rid="pone.0003892-Boynton1">[47]</xref>, event-related power in the high gamma range <xref ref-type="bibr" rid="pone.0003892-TallonBaudry1">[48]</xref> and BOLD fMRI signals <xref ref-type="bibr" rid="pone.0003892-Corbetta1">[49]</xref>. Recently, attention has been shown to improve decoding performance in fMRI <xref ref-type="bibr" rid="pone.0003892-Reddy1">[50]</xref>. Another potential effect of attention is the modulation of communication between separate cortical regions. Recent neurophysiological studies <xref ref-type="bibr" rid="pone.0003892-Fries1">[51]</xref>, <xref ref-type="bibr" rid="pone.0003892-Womelsdorf1">[52]</xref> examined the role of coherence between spikes and local field potentials and showed specific increases in spike-field coherence in the gamma range (∼40 Hz), together with decreases in the beta range (∼20 Hz). Our differential time-frequency decoding map (<xref ref-type="fig" rid="pone-0003892-g007">Figure 7C</xref>), however, revealed rather distributed effects of attention across frequencies. It is worth noting that the peak decoding difference in the time-frequency map (<xref ref-type="fig" rid="pone-0003892-g005">Figure 5C</xref>) was comparable to the peak decoding time course (<xref ref-type="fig" rid="pone-0003892-g005">Figure 5B</xref>), implying no advantage in combining the attentional effects across frequencies. In other words, the attentional effects may be present in broadband, but highly correlated across frequencies. We also note that if attention were to change cross-frequency coherence to improve inter-areal communication (e.g., via modulating signal/noise correlation <xref ref-type="bibr" rid="pone.0003892-Averbeck1">[53]</xref>), the attentional effects for decoding across frequencies and channels (<xref ref-type="fig" rid="pone-0003892-g005">Figure 5B</xref>) would be higher than the attentional effects for decoding across channels at each time-frequency (<xref ref-type="fig" rid="pone-0003892-g005">Figure 5C</xref>), which was not the case. This type of effect is expected if the power of the field potential is modulated uniformly across frequencies. It is tempting to suggest that our observed effects may reflect an increase in firing rate without specific oscillatory components.</p>
</sec><sec id="s3d">
<title>Lack of attentional effects on emotion &amp; gender discrimination</title>
<p>Though attention improved the decoding accuracy for faces vs. checkerboards (A'<sub>fc</sub>), it did not modulate the decoding accuracy for emotion (A'<sub>em</sub>) or gender (A'<sub>gn</sub>) during either the static or morph periods. The lack of attentional modulation for gender decoding (A'<sub>gn</sub>) may be due to floor effects; we did not find good decoding accuracy for gender discrimination in any recording location (<xref ref-type="fig" rid="pone-0003892-g006">Figure 6</xref>), time window, or frequency band. Another possible reason is that our gender task was too easy to engage any attentional effects (behavioral accuracy was 95% correct). In fact, the task can be performed by seeing the stimulus only briefly at any time point during the 2.5 seconds of stimulus presentation, possibly resulting in a temporal spread of attentional effects that are inconsistent across trials and subjects.</p>
<p>By contrast to gender discrimination, emotion discrimination required more temporally focal attention, especially around the onset of the morph epoch, improving the decoding of faces vs. checkerboards (A'<sub>fc</sub>) (<xref ref-type="fig" rid="pone-0003892-g005">Figure 5</xref>). As the emotion decoding (A'<sub>em</sub>) was above chance in both the ventral and the lateral temporal cortex (<xref ref-type="fig" rid="pone-0003892-g004">Figure 4</xref>), the lack of attentional effects are unlikely to be due to floor effects. Again, the task might have been too easy to observe any attentional effects (behavioral A' was 0.90 for emotion discrimination). Using five different categories of emotional faces (which is presumably more attentionally demanding than our stimulus set), Krolak-Salmon et al (2002) reported strong attentional effects of emotion processing in event-related potentials recorded intracranially from the amygdala. Future studies utilizing more demanding tasks or more emotion categories would be necessary to reveal the nature of attentional modulation in the ventral temporal cortex.</p>
</sec><sec id="s3e">
<title>Value of the decoding approach for intracranial EEG</title>
<p>The direct advantages of decoding for intracranial EEG are three-fold: 1) It allows visualization and provides a concise summary of high dimensional data, and is thus especially well-suited for time-frequency analyses of multi-channel recordings; 2) It avoids severe multiple comparison problems inherent to multi-channel time-frequency analyses, which often lead to a rather arbitrary selection of a set of particular electrodes, particular frequency bands, and particular time ranges to be statistically evaluated; 3) It facilitates averaging of data from multiple subjects. In intracranial recordings, electrode distributions as well as the exact locations of sensitive electrodes vary across subjects. With searchlight decoding we combined neighboring electrodes, and with the time-frequency decoding map and time course of decoding we combined all the electrodes within a larger anatomical unit. These operations optimally and linearly blurred fine anatomical structures, making the decoding performance comparable across subjects despite their inhomogeneity. We believe this is an alternative powerful analysis method, useful for future studies, when one is interested in questions at the system level with precise time-frequency resolution.</p>
<p>Those three benefits of the decoding approach are interrelated. We provided an example of the classical analysis in <xref ref-type="fig" rid="pone-0003892-g006">Figure 6A and B</xref>, where these problems can be easily appreciated. To present a spatial map of the time-frequency response, a considerable amount of information gained from the time-frequency analysis is simply wasted due to averaging across time and frequency. In <xref ref-type="fig" rid="pone-0003892-g006">Figure 6A and B</xref>, we summed the evoked power above or below an arbitrary frequency (i.e., 50 Hz), but this is clearly not the optimal strategy. Even if one finds an optimal selection of frequency bands, time points, and spatial locations for averaging, this selection tends to ‘over-fit’ to a particular data set, which generalizes poorly to different subjects. We overcame this problem by optimally linearly combining the response along frequency and time for each subject with an objective and automatic decoding procedure and evaluating the decoder's performance on the test trials, which the decoder did not see during training. Although we lost some spatial specificity (including the polarity of the response only visible in <xref ref-type="fig" rid="pone-0003892-g006">Figure 6A and B</xref>), combining electrodes ‘blurred’ fine spatial structure optimally and linearly and permitted pooling across subjects. A similar problem arises in high-resolution fMRI, where fine spatial patterns of the response make it difficult to average across subjects <xref ref-type="bibr" rid="pone.0003892-Kriegeskorte1">[54]</xref>, <xref ref-type="bibr" rid="pone.0003892-Kriegeskorte2">[55]</xref>. In other words, in both intracranial ECoG and high-resolution fMRI, the spatial resolution is much finer than the spatial jitter inherent to individual anatomical differences. If simple smoothing were used, the very advantage conferred by high spatial resolution is totally discarded. Even though spatial specificity of the response is best preserved in the raw data for each individual subject, we cannot generalize and replicate such a finding to other individuals; we therefore opted for better averaging across subjects at the expense of too fine spatial resolution. The same problems arise for the high temporal resolution of the ECoG. The very advantage of high time-frequency resolution is wasted if one simply averages across time and frequency. The decoding technique on which we capitalized in our study is a powerful alternative for analyzing multi-channel field potentials across individuals by preserving high spatio-temporal resolution with minimal assumptions about timing, frequency and spatial locations of interest.</p>
<p>Finally, we point out the general advantage of decoding analyses: decoding performance (such as A' = 0.8 or 80% correct classification) is intuitive and objective. Compared to decoding analyses, conventional statistical analyses can be difficult to interpret because many factors affect the resulting estimates of significance (e.g., whether or not the assumptions of the response distribution are met, how many subjects and trials are tested, whether there were correlations among the data, etc). This is especially true when multiple factors are considered in a multivariate analysis, where one can easily over-fit the data. Decoding analyses prevent over-fitting with a cross validation procedure (i.e., separate training and test trials) and offers a very intuitive ‘accuracy’ measure, such as % correct or A'. Modern sensory neurophysiology, for example, compares different models of neuronal response within a decoding framework <xref ref-type="bibr" rid="pone.0003892-Wu1">[56]</xref>. As the analysis of electrophysiological data becomes more sophisticated, the intuitive and objective nature of a decoding approach becomes increasingly important. For example, we might be able to improve decoding performance by devising an optimal exclusion criterion for trials and/or electrodes. Similarly, we could objectively compare different kinds of preprocessing techniques, such as source modeling and independent component analysis, and quantify the degree of improvement afforded by each of these. Decoding analyses not only facilitate comparisons across different neuronal measures (such as EEG, MEG, fMRI), physiological measures (such as eye movements, skin conductance), and different aspects of a particular measure (such as the event-related power, the phase, and the degree of synchrony of multiple ECoG) but they ultimately allow us to combine these measures to provide the best inference of our mental life from a third-person perspective.</p>
</sec></sec><sec id="s4">
<title>Materials and Methods</title>
<sec id="s4a">
<title>Subjects</title>
<p>We obtained written informed consent from nine patients with medically intractable epilepsy (see <xref ref-type="table" rid="pone-0003892-t002">Table 2</xref> for detailed demographic information) who were undergoing epilepsy monitoring to guide neurosurgical treatment. The study was approved by the Internal Review Board at the University of Iowa. The patients underwent electrode implantation under a purely clinical protocol and the location of electrodes was determined solely by medical considerations. All patients were on anticonvulsant medications in reduced or absent dosage to facilitate the occurrence of seizures to aid in the clinical detection of the seizure foci. The experiments reported here were conducted typically 6–10 days after the implantation of the electrodes. Recording sessions were kept as brief as possible and were dependent on the patient's willingness for research participation at a given moment as well as on clinical constraints. We did not record when our experiments introduced any clinical inconvenience, and we did not record for 24 hours after any major seizure.</p>
<table-wrap id="pone-0003892-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0003892.t002</object-id><label>Table 2</label><caption>
<title>Patients' demographic information.</title>
</caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pone-0003892-t002-2" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0003892.t002" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" colspan="1" rowspan="1">ID</td>
<td align="left" colspan="1" rowspan="1">Age</td>
<td align="left" colspan="1" rowspan="1">Sex</td>
<td align="left" colspan="1" rowspan="1">Education</td>
<td align="left" colspan="1" rowspan="1">Handedness</td>
<td align="left" colspan="1" rowspan="1">Language</td>
<td align="left" colspan="1" rowspan="1">Side of Grid</td>
<td align="left" colspan="1" rowspan="1">Seizure focus</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">138</td>
<td align="left" colspan="1" rowspan="1">20</td>
<td align="left" colspan="1" rowspan="1">M</td>
<td align="left" colspan="1" rowspan="1">14</td>
<td align="left" colspan="1" rowspan="1">R</td>
<td align="left" colspan="1" rowspan="1">L</td>
<td align="left" colspan="1" rowspan="1">L</td>
<td align="left" colspan="1" rowspan="1">Left anterior lateral temporal, independent right mesial temporal interictal discharge</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">139</td>
<td align="left" colspan="1" rowspan="1">53</td>
<td align="left" colspan="1" rowspan="1">F</td>
<td align="left" colspan="1" rowspan="1">12</td>
<td align="left" colspan="1" rowspan="1">L</td>
<td align="left" colspan="1" rowspan="1">R</td>
<td align="left" colspan="1" rowspan="1">R</td>
<td align="left" colspan="1" rowspan="1">Right mesial temporal</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">140</td>
<td align="left" colspan="1" rowspan="1">26</td>
<td align="left" colspan="1" rowspan="1">M</td>
<td align="left" colspan="1" rowspan="1">10</td>
<td align="left" colspan="1" rowspan="1">R</td>
<td align="left" colspan="1" rowspan="1">L</td>
<td align="left" colspan="1" rowspan="1">L</td>
<td align="left" colspan="1" rowspan="1">Left anterior lower parietal</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">142</td>
<td align="left" colspan="1" rowspan="1">33</td>
<td align="left" colspan="1" rowspan="1">F</td>
<td align="left" colspan="1" rowspan="1">12</td>
<td align="left" colspan="1" rowspan="1">R</td>
<td align="left" colspan="1" rowspan="1">L</td>
<td align="left" colspan="1" rowspan="1">R</td>
<td align="left" colspan="1" rowspan="1">Right mesial temporal</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">146</td>
<td align="left" colspan="1" rowspan="1">29</td>
<td align="left" colspan="1" rowspan="1">F</td>
<td align="left" colspan="1" rowspan="1">16</td>
<td align="left" colspan="1" rowspan="1">R</td>
<td align="left" colspan="1" rowspan="1">L</td>
<td align="left" colspan="1" rowspan="1">L</td>
<td align="left" colspan="1" rowspan="1">Left mesial temporal</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">147</td>
<td align="left" colspan="1" rowspan="1">29</td>
<td align="left" colspan="1" rowspan="1">M</td>
<td align="left" colspan="1" rowspan="1">14</td>
<td align="left" colspan="1" rowspan="1">L</td>
<td align="left" colspan="1" rowspan="1">L</td>
<td align="left" colspan="1" rowspan="1">L</td>
<td align="left" colspan="1" rowspan="1">Left posterior ventral temporal cortex</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">149</td>
<td align="left" colspan="1" rowspan="1">22</td>
<td align="left" colspan="1" rowspan="1">M</td>
<td align="left" colspan="1" rowspan="1">11</td>
<td align="left" colspan="1" rowspan="1">L</td>
<td align="left" colspan="1" rowspan="1">Bilateral</td>
<td align="left" colspan="1" rowspan="1">R</td>
<td align="left" colspan="1" rowspan="1">Bilateral mesial temporal</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">153</td>
<td align="left" colspan="1" rowspan="1">31</td>
<td align="left" colspan="1" rowspan="1">F</td>
<td align="left" colspan="1" rowspan="1">15</td>
<td align="left" colspan="1" rowspan="1">R</td>
<td align="left" colspan="1" rowspan="1">L</td>
<td align="left" colspan="1" rowspan="1">R</td>
<td align="left" colspan="1" rowspan="1">Right mesial temporal</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">154</td>
<td align="left" colspan="1" rowspan="1">40</td>
<td align="left" colspan="1" rowspan="1">M</td>
<td align="left" colspan="1" rowspan="1">13</td>
<td align="left" colspan="1" rowspan="1">R</td>
<td align="left" colspan="1" rowspan="1">L</td>
<td align="left" colspan="1" rowspan="1">R</td>
<td align="left" colspan="1" rowspan="1">Right mesial temporal</td>
</tr>
</tbody>
</table></alternatives><table-wrap-foot><fn id="nt102"><p>Education is indicated in years.</p></fn></table-wrap-foot></table-wrap></sec><sec id="s4b">
<title>Anatomical location of the electrodes</title>
<p>On the lateral temporal cortex, all nine subjects had grid electrodes with 64–96 contacts (mean across subjects = 77.6, std 16.3); five had them on the right hemisphere, and four on the left. Inter-electrode distance of the lateral temporal grid was 5 mm. The grids were configured in a rectangular matrix of 4×8, 8×8, or 12×8. The location of the grid was roughly similar across subjects (<xref ref-type="fig" rid="pone-0003892-g001">Figure 1C</xref>). On the ventral temporal cortex, eight subjects had several strip electrodes in each hemisphere: seven subjects had 4–40 contacts (mean 16.5, std 17.5) in the right hemisphere and eight subjects had 4–16 contacts (mean 11.0, std 4.7) in the left hemisphere. The location and number of the strip electrodes varied (<xref ref-type="fig" rid="pone-0003892-g001">Figure 1B</xref>). Ventral electrodes were either 4-contact strip electrodes or 2×8-contact strip-grid electrodes. Inter-electrode distance of the 4 contact strip electrode was 1 cm and that of 2×8 strip electrode was 5 mm.</p>
<p>For each subject, we obtained structural T1-weighted MRI volumes (pre- and post- electrode implantation), CT scans (post-implantation) and digital photos of the electrodes (during surgery, only for the lateral temporal grid electrodes). Coronal slices of the MRI were obtained with 1 mm slice thickness, 0.78×0.78 mm in-plane resolution. Axial slices of the CT scans were obtained with 1 mm slice thickness, 0.45×0.45 mm in-plane resolution. Post-implantation CT scans and pre-implantation MRI were rendered into 3D volumes and co-registered using AFNI (NIMH, Bethesda, MD, USA) and ANALYZE software (version 7.0, AnalyzeDirect, KS, USA) with mutual information maximization.</p>
<p>Because the ventral temporal strip electrodes were not directly visible during surgery, we did not take any digital photographs of them. However, as the contacts on the strip electrodes were not as dense as in the lateral temporal grids, post-implantation CT scans were sufficient to identify the coordinates of the contacts. We transferred these coordinates onto the higher resolution pre-operative MRI for visualization purposes.</p>
<p>For the lateral temporal grid electrodes, the electrodes were denser than those on the strip electrodes. Therefore, after CT-MRI coregistration, we further refined the estimated coordinates of each contact by visually matching the gyral-sulcal pattern of the MRI-based surface rendering with that of digital photographs taken during electrode placement and removal surgeries.</p>
<p>After the locations of electrode contacts were visualized on the 3D anatomical MRI rendering, we obtained 2D projections of the MRI from ventral (<xref ref-type="fig" rid="pone-0003892-g001">Figure 1B</xref>) and lateral (<xref ref-type="fig" rid="pone-0003892-g001">Figure 1C</xref>) views, using in-house programs in MATLAB 7 (Mathworks, MA, USA). Next, we aligned the 2D projection across subjects by translation, rotation and scaling, using the transparent layers in Adobe Photoshop. For the ventral view, we aligned the outlines of the brains for each subject into that of a reference subject whose brain outline is shown in <xref ref-type="fig" rid="pone-0003892-g001">Figure 1B</xref>. For the lateral view, we first flipped the side for the right hemisphere. Then we aligned each brain into the target brain shown in <xref ref-type="fig" rid="pone-0003892-g001">Figure 1C</xref>. With translation, rotation, and scaling, we aligned the conspicuous anatomical landmarks around the temporal surface, including the lateral sulcus, the superior temporal sulcus, and the outline of the inferior frontal lobe and the anterior temporal lobe.</p>
</sec><sec id="s4c">
<title>Electrode density map and searchlight decoding map</title>
<p>We obtained an electrode density (ED) map for each subject by the following equation;<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0003892.e001" xlink:type="simple"/></disp-formula>where <italic>ED(j)</italic> refers to an electrode density map for <italic>j</italic>-th subject, <italic>pos(i)</italic> is a 2D delta function that is zero except at the coordinate [x,y] for an electrode <italic>i</italic>, <italic>i</italic> spans across all the electrodes for <italic>j</italic>-th subject, •• denotes convolution, and <italic>K</italic> is a 2D Gaussian kernel whose full-width-at-half-maximum was the average inter-electrode distance on the 2D projection and whose extent was circular with an radius being the inter-electrode distance.</p>
<p>When averaging the ED maps across subjects (<xref ref-type="fig" rid="pone-0003892-g001">Figure 1D and E</xref>), we used the following equation;<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0003892.e002" xlink:type="simple"/></disp-formula>Thus, the summed pixel values in the average density map for all subjects equal the average number of electrodes across subjects.</p>
<p>We obtained a searchlight decoding map for one subject by the following equation;<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0003892.e003" xlink:type="simple"/></disp-formula>where <italic>SD(j)</italic> refers to a searchlight decoding map for <italic>j</italic>-th subject, <italic>d(i)</italic> is a scalar constant representing the decoding accuracy at the <italic>i</italic>-th electrode (or <italic>i</italic>-th search light) and <italic>pos(i)</italic> is a 2D delta function that is zero except at the center position of the <italic>i</italic>-th electrode (or <italic>i</italic>-th searchlight). We normalized the summed decoding accuracy by <italic>ED(j)</italic> at each pixel. If there is only one electrode (<italic>i</italic> = 1), <italic>SD(j) is d(1)</italic> for the extent of <italic>K</italic> and not defined elsewhere. When there is an overlap between more than two electrodes, <italic>SD(j)</italic> linearly interpolates the decoding accuracy.</p>
<p>We obtained a searchlight decoding map across subjects (<xref ref-type="fig" rid="pone-0003892-g006">Figure 6E, top row</xref>), by the following equation;<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0003892.e004" xlink:type="simple"/></disp-formula></p>
</sec><sec id="s4d">
<title>Electrophysiological recording and stimulus display</title>
<p>ECoG was recorded with intracranially implanted electrodes (Ad-Tech Medical Instrument Corp., WI, USA). Electrical potential at each electrode was referenced to the electrode placed under the scalp near the vertex of the skull. The impedances of the electrodes were 5 k–20 k Ohm. Signals from the brain were digitized and recorded using the Multi-Channel Neurophysiology Workstation (Tucker-Davis Technologies, FL, USA) and analyzed offline using custom programs in MATLAB. For an initial six subjects, we used an LCD display (Multisync LCD 1760V, NEC, Tokyo, Japan) for stimulus presentation and recorded the electrophysiological signal at a sampling rate of 1 kHz. For the three latest subjects, we used another LCD display (VX922, ViewSonic, CA, USA) and recorded the signal at 2 kHz. In both cases, the display refresh rate was 60 Hz. We measured the precise timing of the stimulus onsets by presenting a small white rectangle on the top-left corner of the display and recorded the response of a photodiode directly attached at that corner. The output from the photodiode was recorded along with the electrophysiological responses in the same recording system.</p>
</sec><sec id="s4e">
<title>Stimuli and Task</title>
<p>We used gray scale pictures of neutral, happy and fearful expressions of 4 individuals (2 female) from the Ekman and Friesen set <xref ref-type="bibr" rid="pone.0003892-Ekman1">[57]</xref>. Each face was equated for size, brightness, contrast, and position and framed in an elliptical window using MATLAB. The faces subtended about 7.5×10 deg in visual angle. For the morph movie period of our stimuli, we created 28 images by linear interpolation between neutral and emotional faces (Morph 2.5, Gryphon Software, CA, USA). We presented the stimuli using Psychtoolbox <xref ref-type="bibr" rid="pone.0003892-Brainard1">[58]</xref> and MATLAB 5.2 on a Power Mac G4 running OS 9.</p>
<p>A trial began with a baseline static plaid pattern for 1 second, followed either by a static neutral face or by a radial checkerboard pattern (with black/white square wave modulation at around 12 cycles per face, <xref ref-type="fig" rid="pone-0003892-g001">Figure 1A</xref>). After a further 1 sec (2 seconds total from the trial onset), the static neutral face started to morph into either a fearful or a happy expression, or the radial checkerboard pattern started to expand or contract. The morph period lasted 500 msec. The last frame in the morph movie stayed on for another 1 second. After the stimulus was extinguished, subjects were prompted to make a response to discriminate the stimulus. In a given session, subjects were instructed to discriminate either the emotion or the gender of the face if they saw a face. They were asked to answer ‘other’ if they saw a checkerboard in all sessions. The prompt reminded subjects of the three alternatives as [1, happy; 2, other; 3 fear] in the emotion discrimination sessions and [1, woman; 2, other; 3, man] in the gender discrimination sessions. After the response, the next trial started. We did not put any time constraint on the response time and did not instruct subjects to put any priority over the speed or accuracy.</p>
<p>One session consisted of 200 trials; 80 trials of neutral-to-fearful face morphs (20 for each identity), 80 trials of neutral-to-happy morphs, 40 trials of checkerboard (20 expanding, 20 contracting).</p>
<p>Subject 139 and 146 performed only one session of the emotion discrimination task (subject 146 completed only 55 trials due to a technical problem). We collected electrophysiological data from a total of 22 sessions across subjects, and behavioral responses from a total of 19 sessions (3800 trials).</p>
</sec><sec id="s4f">
<title>Spectrogram analysis</title>
<p>The spectrogram (time resolved Fourier Transform) of the raw EEG signal was estimated using a multi-taper method (<xref ref-type="supplementary-material" rid="pone.0003892.s001">Figure S1</xref> and <xref ref-type="supplementary-material" rid="pone.0003892.s002">S2</xref>, C–E) with a sliding short time window. We used three Slepian data tapers. For decoding analysis, the logarithm of the power spectrum during the baseline period (−1 to −0.5 sec from the onset of the static baseline stimulus) was subtracted to obtain the event-related power at each frequency in each trial. Input to the decoder was, therefore, the log-transformed event-related power. For the analysis of the high-gamma frequency range (up to 300 Hz), we used a time window of 100 msec with a step size of 50 msec. This gave us a half bandwidth (<italic>W</italic>) of 20 Hz: <italic>W</italic> = (<italic>K</italic>+1)/2<italic>T</italic>, with <italic>K</italic> being the number of data tapers, <italic>K</italic> = 3, and <italic>T</italic> being the length of the time window, <italic>T</italic> = 100 msec. For a time window of 500 msec with a step size of 100 msec, a half bandwidth was 4 Hz.</p>
</sec><sec id="s4g">
<title>Decoding procedure (General)</title>
<p>Using a decoding technique, we discriminated face vs. checkerboard, happy vs. fear and male vs. female by combining the event-related power in different ways; the power at each time-frequency point was combined across electrodes in the ventral or lateral temporal cortex for <italic>time-frequency decoding</italic>, (<xref ref-type="fig" rid="pone-0003892-g002">Figure 2 top</xref>, <xref ref-type="fig" rid="pone-0003892-g003">Figure 3A and B</xref>), across frequencies and electrodes in a given region (e.g., the lateral temporal cortex) for <italic>decoding time course</italic> (<xref ref-type="fig" rid="pone-0003892-g002">Figure 2 bottom</xref>, <xref ref-type="fig" rid="pone-0003892-g003">Figure 3C and D</xref>), and across times and frequencies of a few adjacent electrodes for <italic>searchlight decoding</italic> (<xref ref-type="fig" rid="pone-0003892-g006">Figure 6</xref>). Optimal weights (<italic>w</italic>) were estimated by a regularized least square classifier <xref ref-type="bibr" rid="pone.0003892-Poggio1">[7]</xref>, <xref ref-type="bibr" rid="pone.0003892-Hung1">[8]</xref>. Decoding analysis was separately performed for each session in each subject. 70% of trials were randomly selected to train the classifier, and the remaining 30% of trials were used as test trials to evaluate classification performance. In order to minimize the bias of the classifier, we sampled the same number of trials for either class (e.g., face vs. checkerboard); if the numbers of trials differed between the classes (for example, n<sub>1</sub> trials for class 1 and n<sub>2</sub> trials for class 2, where n<sub>1</sub>&gt;n<sub>2</sub>), we first randomly sampled a subset of trials from the class with more trials (i.e., we randomly sampled n<sub>2</sub> trials from class 1) so that the two classes had the same number of trials. This procedure resulted in n<sub>1</sub> = n<sub>2</sub> = 40 for face vs. checker classifier and n<sub>1</sub> = n<sub>2</sub> = 80 for emotion or gender classifier. We applied this general rule to one exceptional session where we had only 55 trials in total. Because we balanced the number of trials for each classes, we did not observe any decoding bias, as is seen for decoding time course in <xref ref-type="fig" rid="pone-0003892-g003">Figure 3</xref>–<xref ref-type="fig" rid="pone-0003892-g004"/><xref ref-type="fig" rid="pone-0003892-g005">5</xref>; A' was at the chance level ( = 0.5) during the baseline period (t&lt;0) in face vs. checkerboard discrimination (<xref ref-type="fig" rid="pone-0003892-g003">Figure 3C and D</xref> and <xref ref-type="fig" rid="pone-0003892-g005">Figure 5A</xref>) and before the morph period (t&lt;1) in emotion discrimination (<xref ref-type="fig" rid="pone-0003892-g004">Figure 4A, B, and E</xref>). When discriminating face vs. checkerboard, we pooled all face trials across different emotions, genders and identity and discriminated those trials from checkerboard trials (collapsing across contraction and expansion epochs). As a result, in an extreme case, a face vs. checkerboard classifier might have been trained on 28 female happy faces and 20 contracting and 8 expanding motion trials (i.e., 70% of n<sub>1</sub> = n<sub>2</sub> = 40 trials is 28 trials) and then tested on 12 male fear faces and 12 expanding motion trials, a more strict test for generalization.</p>
<p>We assessed decoding performance using signal detection theory <xref ref-type="bibr" rid="pone.0003892-Macmillan1">[9]</xref>; we sorted the output from the classifier on the test trials and subjected it to an ROC analysis. We report the non-parametric estimates of sensitivity, area under the ROC curve (A') as an index of decoding performance rather than % correct classification, because A' incorporates information in the magnitude of classifier outputs, which is totally discarded in % correct classification. We repeated the above procedure 10 times to obtain an estimate of A'. A separate classifier was trained and A' was estimated for each classifier at each data point. For example, the time-frequency decoding of emotion shown in <xref ref-type="fig" rid="pone-0003892-g002">Figure 2</xref> (top panel), used 36360 independent classifiers (36 time points×101 frequencies; at each time-frequency point, 10 classifiers were trained with a different set of 70% of trials for training). This was repeated for each of 22 sessions.</p>
<p>Any abnormal trials (i.e., due to apparent epileptic spikes) did not significantly affect our decoding analysis. The aberrant trials in the training set would contribute minimally to the learning of optimal weights because we used a regularized classifier to reduce the effects of outliers <xref ref-type="bibr" rid="pone.0003892-Poggio1">[7]</xref>. Those in the test set could only reduce, not improve, the decoding accuracy. Similarly, bad electrodes or electrodes close to epileptic foci would be expected to affect our analysis only minimally because, during training, those electrodes would automatically be assigned lower weights to improve the decoding accuracy. In other words, our decoding approach automatically and objectively pruned the influence of abnormal trials and electrodes, without relying on any subjective criterion for removal of a subset of trials and electrodes (e.g., apparently large amplitude or apparent epileptic spikes).</p>
<p>When we averaged A's across subjects, we first converted A' into z-scores using a logit transform to normalize the distribution. All statistical tests, except for the paired t-test to quantify the attentional effects (see below), were done on the z-scores. For visualization of results, we transformed the mean and mean±one standard error of the z-scores back to A' with an inverse logit transform.</p>
</sec><sec id="s4h">
<title>Time-frequency decoding</title>
<p>Optimally combining event-related power at each time-frequency point across many channels in a circumscribed anatomical structure, we characterized information at each time-frequency point without any prior assumptions about particular frequency bands and particular time points (<xref ref-type="fig" rid="pone-0003892-g002">Figure 2, top</xref>). For the statistical analysis, we smoothed the decoding map for each subject with a 2D Gaussian kernel in time and frequency (5×5 pixels, with std = 2 pixels) then averaged across subjects. We calculated p-values using two-tailed t-tests against chance (z(A') = 0 or A' = 0.5). We corrected for multiple comparisons using false discovery rate corrections (FDR q&lt;0.1) <xref ref-type="bibr" rid="pone.0003892-Benjamini1">[59]</xref>.</p>
</sec><sec id="s4i">
<title>Time course of decoding</title>
<p>Optimally combining event-related power across frequencies and channels, we characterized information in time in each anatomical region (<xref ref-type="fig" rid="pone-0003892-g002">Figure 2, bottom</xref>). No smoothing was applied in the time dimension to accurately estimate the latency of decoding. P-values from two-tailed t-tests against chance (A' = 0.5) were calculated and corrected for multiple comparisons by FDR (q&lt;0.05). The first time point when the decoding became significant was defined as the latency of decoding (<xref ref-type="table" rid="pone-0003892-t001">Table 1</xref>).</p>
</sec><sec id="s4j">
<title>Attention effects</title>
<p>We analyzed the effects of task-relevant attention by comparing the decoding accuracy between the emotion- and the gender- discrimination sessions. In <xref ref-type="fig" rid="pone-0003892-g005">Figure 5B</xref>, for example, we subtracted the decoding accuracy for discriminating face vs. checkerboard (A'<sub>fc</sub>) in sessions when subjects discriminated the gender from that when they discriminated the emotion for each subject. For this analysis, we used the raw A' for subtraction and performed the paired t-test at each time (<xref ref-type="fig" rid="pone-0003892-g005">Figure 5B</xref>) or time-frequency point (<xref ref-type="fig" rid="pone-0003892-g005">Figure 5C</xref>), because the difference of the raw A' was normally distributed.</p>
<p>4 subjects performed each task once and 3 subjects performed each task twice, once in the uni-directional and once in the bi-directional morph condition. In total, 10 sessions of the emotion task were paired with 10 sessions of the gender task in each subject, equated in the morph direction condition.</p>
</sec><sec id="s4k">
<title>Single-electrode decoding and searchlight decoding</title>
<p>For single-electrode and searchlight decoding (<xref ref-type="fig" rid="pone-0003892-g006">Figure 6</xref>), we combined event-related power during the static period (100–900 msec after the onset of the static stimuli) or the morph period (100–900 msec after the onset of the morphing). To roughly equate the number of inputs to the classifier, we downsampled the event-related power along the frequency dimension by 1/6 for searchlight decoding.</p>
<p>For the lateral temporal grid contacts, the inter-electrode distance was uniform. Thus we used a searchlight with a radius of approximately 5 mm, which covered 4 neighboring electrodes for all subjects. For the ventral lateral cortex, electrodes were placed differently for each subject. In order to retain regional specificity, we used a fixed radius, which was 1/10 of the diameter of the cerebral hemisphere. This radius contained 4 electrodes on average.</p>
</sec><sec id="s4l">
<title>Laterality analysis</title>
<p>For the laterality analysis (<xref ref-type="fig" rid="pone-0003892-g007">Figure 7</xref>), we combined event-related power along frequencies, time (100–900 msec from the onset of the morph period) and electrodes within each hemispheric region. The electrodes were grouped either in the anterior or posterior half of the lateral temporal cortex, and the left or the right hemisphere of the ventral temporal cortex. We downsampled the event-related power along the frequency dimension by 1/6.</p>
</sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pone.0003892.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pone.0003892.s001" xlink:type="simple"><label>Figure S1</label><caption>
<p>Standard ERP and spectrogram analysis (1) A–H) Analysis of channel 75 for subject 153 (session 1). A ERP analysis. The average traces of field potentials are shown for fearful faces (80 trials, thick blue), happy faces (80 trials, thick red) and checkerboards (40 trials, thin black). This electrode showed large positive potential to faces at around 200 msec from the onset of the stimuli (t = 0 sec), but no such clear peak around the onset of the dynamic morph (t = 1 sec). B T-score for the difference between face and checkerboard (red) and between fearful and happy faces (blue). C–E Mean time-frequency spectrogram for fearful (C), happy (D) and checkerboard (E) conditions. Increased power around 100 Hz is seen at just after the onset of both static (t = 0 sec) and morph (t = 1 sec) period for faces (C and D) but not for checkerboards (E). Mean of the spectrogram for each trial is color-coded in log-scale. See the bar at the right for color scale. F and G T-scores for the difference between faces and checkerboards (F) and between fearful and happy faces (G), showing strong difference between conditions in high frequencies, which was not evident in the ERP analysis (A and B). See the bar at the right for color scale. H Relative power increase in the high-gamma bands (50–150 Hz). Mean high gamma power for fearful (blue), happy (red) and checkerboard (black) conditions are plotted, with the shades indicating one standard error of the mean across trials. The high-gamma power for this electrode increased relative to the baseline to faces, but not to checkerboard, at the onset of the static period. During the morph period, it increased even higher for fearful than happy faces.</p>
<p>(3.21 MB TIF)</p>
</caption></supplementary-material><supplementary-material id="pone.0003892.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pone.0003892.s002" xlink:type="simple"><label>Figure S2</label><caption>
<p>Standard ERP and spectrogram analysis (2) A–H) Analysis of channel 74 for subject 153 (session 1). A ERP analysis. This electrode showed much larger negative and positive potential to faces at around 200 msec from the onset of the stimuli than channel 75. B T-score for the difference between face and checkerboard (red) and between fearful and happy faces (blue). C–E Mean time-frequency spectrogram for fearful (C), happy (D) and checkerboard (E) conditions. (E). Mean of the spectrogram. (F and G) T-scores for the difference between faces and checkerboards (F) and between fearful and happy faces (G). See the bar at the right for color scale. H Relative power increase in the high-gamma bands (50–150 Hz). Mean high gamma power for fearful (blue), happy (red) and checkerboard (black) conditions are plotted, with the shades indicating one standard error of the mean across trials.</p>
<p>(3.43 MB TIF)</p>
</caption></supplementary-material><supplementary-material id="pone.0003892.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pone.0003892.s003" xlink:type="simple"><label>Figure S3</label><caption>
<p>Electrode 74 and 75 are marked by green and blue circles, respectively, in the right ventral temporal cortex.</p>
<p>(8.22 MB TIF)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>We thank Haiming Chen for overall technical assistance, Yota Kimura, Christopher Kovach and Joe Hitchon for their assistance during various phases of the experiment, Dirk Neumann and Ueli Rutishauser for helpful discussion on the analysis, and Alex Maier, Rufin VanRullen, Christof Koch and Fred Gosselin for their comments on the manuscript. We thank all subjects who participated in the study for their time.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0003892-Allison1"><label>1</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Allison</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Puce</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Spencer</surname><given-names>DD</given-names></name>
<name name-style="western"><surname>McCarthy</surname><given-names>G</given-names></name>
</person-group>             <year>1999</year>             <article-title>Electrophysiological studies of human face perception. I: Potentials generated in occipitotemporal cortex by face and non-face stimuli.</article-title>             <source>Cereb Cortex</source>             <volume>9</volume>             <issue>(5)</issue>             <fpage>415</fpage>             <lpage>430</lpage>          </element-citation></ref>
<ref id="pone.0003892-McCarthy1"><label>2</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>McCarthy</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Puce</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Belger</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Allison</surname><given-names>T</given-names></name>
</person-group>             <year>1999</year>             <article-title>Electrophysiological studies of human face perception. II: Response properties of face-specific potentials generated in occipitotemporal cortex.</article-title>             <source>Cereb Cortex</source>             <volume>9</volume>             <issue>(5)</issue>             <fpage>431</fpage>             <lpage>444</lpage>          </element-citation></ref>
<ref id="pone.0003892-Puce1"><label>3</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Puce</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Allison</surname><given-names>T</given-names></name>
<name name-style="western"><surname>McCarthy</surname><given-names>G</given-names></name>
</person-group>             <year>1999</year>             <article-title>Electrophysiological studies of human face perception. III: Effects of top-down processing on face-specific potentials.</article-title>             <source>Cereb Cortex</source>             <volume>9</volume>             <issue>(5)</issue>             <fpage>445</fpage>             <lpage>458</lpage>          </element-citation></ref>
<ref id="pone.0003892-Halgren1"><label>4</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Halgren</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Baudena</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Heit</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Clarke</surname><given-names>JM</given-names></name>
<name name-style="western"><surname>Marinkovic</surname><given-names>K</given-names></name>
<etal/></person-group>             <year>1994</year>             <article-title>Spatio-temporal stages in face and word processing. I. Depth-recorded potentials in the human occipital, temporal and parietal lobes [corrected].</article-title>             <source>Journal of physiology, Paris</source>             <volume>88</volume>             <issue>(1)</issue>             <fpage>1</fpage>             <lpage>50</lpage>          </element-citation></ref>
<ref id="pone.0003892-Mitra1"><label>5</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Mitra</surname><given-names>PP</given-names></name>
<name name-style="western"><surname>Pesaran</surname><given-names>B</given-names></name>
</person-group>             <year>1999</year>             <article-title>Analysis of dynamic brain imaging data.</article-title>             <source>Biophysical journal</source>             <volume>76</volume>             <issue>(2)</issue>             <fpage>691</fpage>             <lpage>708</lpage>          </element-citation></ref>
<ref id="pone.0003892-Pesaran1"><label>6</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Pesaran</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Pezaris</surname><given-names>JS</given-names></name>
<name name-style="western"><surname>Sahani</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Mitra</surname><given-names>PP</given-names></name>
<name name-style="western"><surname>Andersen</surname><given-names>RA</given-names></name>
</person-group>             <year>2002</year>             <article-title>Temporal structure in neuronal activity during working memory in macaque parietal cortex.</article-title>             <source>Nat Neurosci</source>             <volume>5</volume>             <issue>(8)</issue>             <fpage>805</fpage>             <lpage>811</lpage>          </element-citation></ref>
<ref id="pone.0003892-Poggio1"><label>7</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Smale</surname><given-names>S</given-names></name>
</person-group>             <year>2003</year>             <article-title>The Mathematicss of Learning: Dealing with Data.</article-title>             <source>Notices of the American Mathematics Society</source>             <volume>50</volume>             <issue>(5)</issue>             <fpage>537</fpage>             <lpage>544</lpage>          </element-citation></ref>
<ref id="pone.0003892-Hung1"><label>8</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hung</surname><given-names>CP</given-names></name>
<name name-style="western"><surname>Kreiman</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name>
<name name-style="western"><surname>DiCarlo</surname><given-names>JJ</given-names></name>
</person-group>             <year>2005</year>             <article-title>Fast readout of object identity from macaque inferior temporal cortex.</article-title>             <source>Science</source>             <volume>310</volume>             <issue>(5749)</issue>             <fpage>863</fpage>             <lpage>866</lpage>          </element-citation></ref>
<ref id="pone.0003892-Macmillan1"><label>9</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Macmillan</surname><given-names>NA</given-names></name>
<name name-style="western"><surname>Creelman</surname><given-names>CD</given-names></name>
</person-group>             <year>1991</year>             <source>Detection Theory: A User's Guide</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>Cambridge University Press</publisher-name>          </element-citation></ref>
<ref id="pone.0003892-Logothetis1"><label>10</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Logothetis</surname><given-names>NK</given-names></name>
<name name-style="western"><surname>Pauls</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Augath</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Trinath</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Oeltermann</surname><given-names>A</given-names></name>
</person-group>             <year>2001</year>             <article-title>Neurophysiological investigation of the basis of the fMRI signal.</article-title>             <source>Nature</source>             <volume>412</volume>             <issue>(6843)</issue>             <fpage>150</fpage>             <lpage>157</lpage>          </element-citation></ref>
<ref id="pone.0003892-Nir1"><label>11</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Nir</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Fisch</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Mukamel</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Gelbard-Sagiv</surname><given-names>H</given-names></name>
<name name-style="western"><surname>Arieli</surname><given-names>A</given-names></name>
<etal/></person-group>             <year>2007</year>             <article-title>Coupling between neuronal firing rate, gamma LFP, and BOLD fMRI is related to interneuronal correlations.</article-title>             <source>Curr Biol</source>             <volume>17</volume>             <issue>(15)</issue>             <fpage>1275</fpage>             <lpage>1285</lpage>          </element-citation></ref>
<ref id="pone.0003892-Nir2"><label>12</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Nir</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Mukamel</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Dinstein</surname><given-names>I</given-names></name>
<name name-style="western"><surname>Privman</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Harel</surname><given-names>M</given-names></name>
<etal/></person-group>             <year>2008</year>             <article-title>Interhemispheric correlations of slow spontaneous neuronal fluctuations revealed in human sensory cortex.</article-title>             <source>Nat Neurosci</source>          </element-citation></ref>
<ref id="pone.0003892-Haxby1"><label>13</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Haxby</surname><given-names>JV</given-names></name>
<name name-style="western"><surname>Hoffman</surname><given-names>EA</given-names></name>
<name name-style="western"><surname>Gobbini</surname><given-names>MI</given-names></name>
</person-group>             <year>2000</year>             <article-title>The distributed human neural system for face perception.</article-title>             <source>Trends Cogn Sci (Regul Ed)</source>             <fpage>223</fpage>             <lpage>233</lpage>          </element-citation></ref>
<ref id="pone.0003892-Allison2"><label>14</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Allison</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Puce</surname><given-names>A</given-names></name>
<name name-style="western"><surname>McCarthy</surname><given-names>G</given-names></name>
</person-group>             <year>2000</year>             <article-title>Social perception from visual cues: role of the STS region.</article-title>             <source>Trends Cogn Sci</source>             <volume>4</volume>             <issue>(7)</issue>             <fpage>267</fpage>             <lpage>278</lpage>          </element-citation></ref>
<ref id="pone.0003892-Schyns1"><label>15</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Schyns</surname><given-names>PG</given-names></name>
<name name-style="western"><surname>Petro</surname><given-names>LS</given-names></name>
<name name-style="western"><surname>Smith</surname><given-names>ML</given-names></name>
</person-group>             <year>2007</year>             <article-title>Dynamics of visual information integration in the brain for categorizing facial expressions.</article-title>             <source>Curr Biol</source>             <volume>17</volume>             <issue>(18)</issue>             <fpage>1580</fpage>             <lpage>1585</lpage>          </element-citation></ref>
<ref id="pone.0003892-Smith1"><label>16</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Smith</surname><given-names>ML</given-names></name>
<name name-style="western"><surname>Cottrell</surname><given-names>GW</given-names></name>
<name name-style="western"><surname>Gosselin</surname><given-names>F</given-names></name>
<name name-style="western"><surname>Schyns</surname><given-names>PG</given-names></name>
</person-group>             <year>2005</year>             <article-title>Transmitting and decoding facial expressions.</article-title>             <source>Psychol Sci</source>             <volume>16</volume>             <issue>(3)</issue>             <fpage>184</fpage>             <lpage>189</lpage>          </element-citation></ref>
<ref id="pone.0003892-Calder1"><label>17</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Calder</surname><given-names>AJ</given-names></name>
<name name-style="western"><surname>Young</surname><given-names>AW</given-names></name>
</person-group>             <year>2005</year>             <article-title>Understanding the recognition of facial identity and facial expression.</article-title>             <source>Nat Rev Neurosci</source>             <fpage>641</fpage>             <lpage>651</lpage>          </element-citation></ref>
<ref id="pone.0003892-DeRenzi1"><label>18</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>De Renzi</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Perani</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Carlesimo</surname><given-names>GA</given-names></name>
<name name-style="western"><surname>Silveri</surname><given-names>MC</given-names></name>
<name name-style="western"><surname>Fazio</surname><given-names>F</given-names></name>
</person-group>             <year>1994</year>             <article-title>Prosopagnosia can be associated with damage confined to the right hemisphere–an MRI and PET study and a review of the literature.</article-title>             <source>Neuropsychologia</source>             <volume>32</volume>             <issue>(8)</issue>             <fpage>893</fpage>             <lpage>902</lpage>          </element-citation></ref>
<ref id="pone.0003892-Adolphs1"><label>19</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Adolphs</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Damasio</surname><given-names>H</given-names></name>
<name name-style="western"><surname>Tranel</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Cooper</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Damasio</surname><given-names>AR</given-names></name>
</person-group>             <year>2000</year>             <article-title>A role for somatosensory cortices in the visual recognition of emotion as revealed by three-dimensional lesion mapping.</article-title>             <source>J Neurosci</source>             <volume>20</volume>             <issue>(7)</issue>             <fpage>2683</fpage>             <lpage>2690</lpage>          </element-citation></ref>
<ref id="pone.0003892-McCarthy2"><label>20</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>McCarthy</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Puce</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Gore</surname><given-names>JC</given-names></name>
<name name-style="western"><surname>Allison</surname><given-names>T</given-names></name>
</person-group>             <year>1997</year>             <article-title>Face-specific processing in the human fusiform gyrus.</article-title>             <source>Journal of Cognitive Neuroscience</source>             <volume>9</volume>             <fpage>605</fpage>             <lpage>610</lpage>          </element-citation></ref>
<ref id="pone.0003892-Fried1"><label>21</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Fried</surname><given-names>I</given-names></name>
<name name-style="western"><surname>Mateer</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Ojemann</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Wohns</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Fedio</surname><given-names>P</given-names></name>
</person-group>             <year>1982</year>             <article-title>Organization of visuospatial functions in human cortex. Evidence from electrical stimulation.</article-title>             <source>Brain</source>             <fpage>349</fpage>             <lpage>371</lpage>          </element-citation></ref>
<ref id="pone.0003892-Morris1"><label>22</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Morris</surname><given-names>JS</given-names></name>
<name name-style="western"><surname>Frith</surname><given-names>CD</given-names></name>
<name name-style="western"><surname>Perrett</surname><given-names>DI</given-names></name>
<name name-style="western"><surname>Rowland</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Young</surname><given-names>AW</given-names></name>
<etal/></person-group>             <year>1996</year>             <article-title>A differential neural response in the human amygdala to fearful and happy facial expressions.</article-title>             <source>Nature</source>             <volume>383</volume>             <issue>(6603)</issue>             <fpage>812</fpage>             <lpage>815</lpage>          </element-citation></ref>
<ref id="pone.0003892-Sergent1"><label>23</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sergent</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Ohta</surname><given-names>S</given-names></name>
<name name-style="western"><surname>MacDonald</surname><given-names>B</given-names></name>
</person-group>             <year>1992</year>             <article-title>Functional neuroanatomy of face and object processing. A positron emission tomography study.</article-title>             <source>Brain</source>             <volume>115</volume>             <issue>Pt 1</issue>             <fpage>15</fpage>             <lpage>36</lpage>          </element-citation></ref>
<ref id="pone.0003892-Kanwisher1"><label>24</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kanwisher</surname><given-names>N</given-names></name>
<name name-style="western"><surname>McDermott</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Chun</surname><given-names>MM</given-names></name>
</person-group>             <year>1997</year>             <article-title>The fusiform face area: a module in human extrastriate cortex specialized for face perception.</article-title>             <source>J Neurosci</source>             <volume>17</volume>             <issue>(11)</issue>             <fpage>4302</fpage>             <lpage>4311</lpage>          </element-citation></ref>
<ref id="pone.0003892-LaBar1"><label>25</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>LaBar</surname><given-names>KS</given-names></name>
<name name-style="western"><surname>Crupain</surname><given-names>MJ</given-names></name>
<name name-style="western"><surname>Voyvodic</surname><given-names>JT</given-names></name>
<name name-style="western"><surname>McCarthy</surname><given-names>G</given-names></name>
</person-group>             <year>2003</year>             <article-title>Dynamic perception of facial affect and identity in the human brain.</article-title>             <source>Cereb Cortex</source>             <volume>13</volume>             <issue>(10)</issue>             <fpage>1023</fpage>             <lpage>1033</lpage>          </element-citation></ref>
<ref id="pone.0003892-Sato1"><label>26</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sato</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Kochiyama</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Yoshikawa</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Naito</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Matsumura</surname><given-names>M</given-names></name>
</person-group>             <year>2004</year>             <article-title>Enhanced neural activity in response to dynamic facial expressions of emotion: an fMRI study.</article-title>             <source>Brain Res Cogn Brain Res</source>             <volume>20</volume>             <issue>(1)</issue>             <fpage>81</fpage>             <lpage>91</lpage>          </element-citation></ref>
<ref id="pone.0003892-Bentin1"><label>27</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bentin</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Allison</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Puce</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Perez</surname><given-names>E</given-names></name>
<name name-style="western"><surname>McCarthy</surname><given-names>G</given-names></name>
</person-group>             <year>1996</year>             <article-title>Electrophysiological studies of face perception in humans.</article-title>             <source>Journal of Cognitive Neuroscience</source>             <volume>8</volume>             <fpage>551</fpage>             <lpage>565</lpage>          </element-citation></ref>
<ref id="pone.0003892-Seeck1"><label>28</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Seeck</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Michel</surname><given-names>CM</given-names></name>
<name name-style="western"><surname>Mainwaring</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Cosgrove</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Blume</surname><given-names>H</given-names></name>
<etal/></person-group>             <year>1997</year>             <article-title>Evidence for rapid face recognition from human scalp and intracranial electrodes.</article-title>             <source>Neuroreport</source>             <volume>8</volume>             <issue>(12)</issue>             <fpage>2749</fpage>             <lpage>2754</lpage>          </element-citation></ref>
<ref id="pone.0003892-Eimer1"><label>29</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Eimer</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Holmes</surname><given-names>A</given-names></name>
</person-group>             <year>2007</year>             <article-title>Event-related brain potential correlates of emotional face processing.</article-title>             <source>Neuropsychologia</source>             <volume>45</volume>             <issue>(1)</issue>             <fpage>15</fpage>             <lpage>31</lpage>          </element-citation></ref>
<ref id="pone.0003892-MouchetantRostaing1"><label>30</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Mouchetant-Rostaing</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Giard</surname><given-names>MH</given-names></name>
<name name-style="western"><surname>Bentin</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Aguera</surname><given-names>PE</given-names></name>
<name name-style="western"><surname>Pernier</surname><given-names>J</given-names></name>
</person-group>             <year>2000</year>             <article-title>Neurophysiological correlates of face gender processing in humans.</article-title>             <source>Eur J Neurosci</source>             <volume>12</volume>             <issue>(1)</issue>             <fpage>303</fpage>             <lpage>310</lpage>          </element-citation></ref>
<ref id="pone.0003892-Pizzagalli1"><label>31</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Pizzagalli</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Regard</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Lehmann</surname><given-names>D</given-names></name>
</person-group>             <year>1999</year>             <article-title>Rapid emotional face processing in the human right and left brain hemispheres: an ERP study.</article-title>             <source>Neuroreport</source>             <volume>10</volume>             <issue>(13)</issue>             <fpage>2691</fpage>             <lpage>2698</lpage>          </element-citation></ref>
<ref id="pone.0003892-Liu1"><label>32</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Liu</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Harris</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Kanwisher</surname><given-names>N</given-names></name>
</person-group>             <year>2002</year>             <article-title>Stages of processing in face perception: an MEG study.</article-title>             <source>Nat Neurosci</source>             <volume>5</volume>             <issue>(9)</issue>             <fpage>910</fpage>             <lpage>916</lpage>          </element-citation></ref>
<ref id="pone.0003892-Halgren2"><label>33</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Halgren</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Raij</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Marinkovic</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Jousmaki</surname><given-names>V</given-names></name>
<name name-style="western"><surname>Hari</surname><given-names>R</given-names></name>
</person-group>             <year>2000</year>             <article-title>Cognitive response profile of the human fusiform face area as determined by MEG.</article-title>             <source>Cereb Cortex</source>             <volume>10</volume>             <issue>(1)</issue>             <fpage>69</fpage>             <lpage>81</lpage>          </element-citation></ref>
<ref id="pone.0003892-KrolakSalmon1"><label>34</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Krolak-Salmon</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Henaff</surname><given-names>MA</given-names></name>
<name name-style="western"><surname>Vighetto</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Bertrand</surname><given-names>O</given-names></name>
<name name-style="western"><surname>Mauguiere</surname><given-names>F</given-names></name>
</person-group>             <year>2004</year>             <article-title>Early amygdala reaction to fear spreading in occipital, temporal, and frontal cortex: a depth electrode ERP study in human.</article-title>             <source>Neuron</source>             <volume>42</volume>             <issue>(4)</issue>             <fpage>665</fpage>             <lpage>676</lpage>          </element-citation></ref>
<ref id="pone.0003892-Lachaux1"><label>35</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Lachaux</surname><given-names>JP</given-names></name>
<name name-style="western"><surname>George</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Tallon-Baudry</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Martinerie</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Hugueville</surname><given-names>L</given-names></name>
<etal/></person-group>             <year>2005</year>             <article-title>The many faces of the gamma band response to complex visual stimuli.</article-title>             <source>Neuroimage</source>             <volume>25</volume>             <issue>(2)</issue>             <fpage>491</fpage>             <lpage>501</lpage>          </element-citation></ref>
<ref id="pone.0003892-Bruce1"><label>36</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bruce</surname><given-names>V</given-names></name>
<name name-style="western"><surname>Young</surname><given-names>A</given-names></name>
</person-group>             <year>1986</year>             <article-title>Understanding face recognition.</article-title>             <source>British journal of psychology (London, England : 1953)</source>             <fpage>305</fpage>             <lpage>327</lpage>          </element-citation></ref>
<ref id="pone.0003892-Kanwisher2"><label>37</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kanwisher</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Yovel</surname><given-names>G</given-names></name>
</person-group>             <year>2006</year>             <article-title>The fusiform face area: a cortical region specialized for the perception of faces.</article-title>             <source>Philos Trans R Soc Lond B Biol Sci</source>             <volume>361</volume>             <issue>(1476)</issue>             <fpage>2109</fpage>             <lpage>2128</lpage>          </element-citation></ref>
<ref id="pone.0003892-Adolphs2"><label>38</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Adolphs</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Tranel</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Hamann</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Young</surname><given-names>AW</given-names></name>
<name name-style="western"><surname>Calder</surname><given-names>AJ</given-names></name>
<etal/></person-group>             <year>1999</year>             <article-title>Recognition of facial emotion in nine individuals with bilateral amygdala damage.</article-title>             <source>Neuropsychologia</source>             <volume>37</volume>             <issue>(10)</issue>             <fpage>1111</fpage>             <lpage>1117</lpage>          </element-citation></ref>
<ref id="pone.0003892-Phillips1"><label>39</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Phillips</surname><given-names>ML</given-names></name>
<name name-style="western"><surname>Young</surname><given-names>AW</given-names></name>
<name name-style="western"><surname>Senior</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Brammer</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Andrew</surname><given-names>C</given-names></name>
<etal/></person-group>             <year>1997</year>             <article-title>A specific neural substrate for perceiving facial expressions of disgust.</article-title>             <source>Nature</source>             <volume>389</volume>             <issue>(6650)</issue>             <fpage>495</fpage>             <lpage>498</lpage>          </element-citation></ref>
<ref id="pone.0003892-Sugase1"><label>40</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sugase</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Yamane</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Ueno</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Kawano</surname><given-names>K</given-names></name>
</person-group>             <year>1999</year>             <article-title>Global and fine information coded by single neurons in the temporal visual cortex.</article-title>             <source>Nature</source>             <volume>400</volume>             <issue>(6747)</issue>             <fpage>869</fpage>             <lpage>873</lpage>          </element-citation></ref>
<ref id="pone.0003892-Young1"><label>41</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Young</surname><given-names>MP</given-names></name>
<name name-style="western"><surname>Yamane</surname><given-names>S</given-names></name>
</person-group>             <year>1992</year>             <article-title>Sparse population coding of faces in the inferotemporal cortex.</article-title>             <source>Science</source>             <volume>256</volume>             <issue>(5061)</issue>             <fpage>1327</fpage>             <lpage>1331</lpage>          </element-citation></ref>
<ref id="pone.0003892-Mitzdorf1"><label>42</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Mitzdorf</surname><given-names>U</given-names></name>
</person-group>             <year>1985</year>             <article-title>Current source-density method and application in cat cerebral cortex: investigation of evoked potentials and EEG phenomena.</article-title>             <source>Physiological reviews</source>             <volume>65</volume>             <issue>(1)</issue>             <fpage>37</fpage>             <lpage>100</lpage>          </element-citation></ref>
<ref id="pone.0003892-VanRullen1"><label>43</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>VanRullen</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Thorpe</surname><given-names>SJ</given-names></name>
</person-group>             <year>2001</year>             <article-title>The time course of visual processing: from early perception to decision-making.</article-title>             <source>J Cogn Neurosci</source>             <volume>13</volume>             <issue>(4)</issue>             <fpage>454</fpage>             <lpage>461</lpage>          </element-citation></ref>
<ref id="pone.0003892-Foxe1"><label>44</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Foxe</surname><given-names>JJ</given-names></name>
<name name-style="western"><surname>Simpson</surname><given-names>GV</given-names></name>
</person-group>             <year>2002</year>             <article-title>Flow of activation from V1 to frontal cortex in humans. A framework for defining “early” visual processing.</article-title>             <source>Exp Brain Res</source>             <volume>142</volume>             <issue>(1)</issue>             <fpage>139</fpage>             <lpage>150</lpage>          </element-citation></ref>
<ref id="pone.0003892-Canolty1"><label>45</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Canolty</surname><given-names>RT</given-names></name>
<name name-style="western"><surname>Edwards</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Dalal</surname><given-names>SS</given-names></name>
<name name-style="western"><surname>Soltani</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Nagarajan</surname><given-names>SS</given-names></name>
<etal/></person-group>             <year>2006</year>             <article-title>High gamma power is phase-locked to theta oscillations in human neocortex.</article-title>             <source>Science</source>             <volume>313</volume>             <issue>(5793)</issue>             <fpage>1626</fpage>             <lpage>1628</lpage>          </element-citation></ref>
<ref id="pone.0003892-Desimone1"><label>46</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Desimone</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Duncan</surname><given-names>J</given-names></name>
</person-group>             <year>1995</year>             <article-title>Neural mechanisms of selective visual attention.</article-title>             <source>Annu Rev Neurosci</source>             <volume>18</volume>             <fpage>193</fpage>             <lpage>222</lpage>          </element-citation></ref>
<ref id="pone.0003892-Boynton1"><label>47</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Boynton</surname><given-names>GM</given-names></name>
</person-group>             <year>2005</year>             <article-title>Attention and visual perception.</article-title>             <source>Curr Opin Neurobiol</source>             <volume>15</volume>             <issue>(4)</issue>             <fpage>465</fpage>             <lpage>469</lpage>          </element-citation></ref>
<ref id="pone.0003892-TallonBaudry1"><label>48</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tallon-Baudry</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Bertrand</surname><given-names>O</given-names></name>
<name name-style="western"><surname>Henaff</surname><given-names>MA</given-names></name>
<name name-style="western"><surname>Isnard</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Fischer</surname><given-names>C</given-names></name>
</person-group>             <year>2005</year>             <article-title>Attention modulates gamma-band oscillations differently in the human lateral occipital cortex and fusiform gyrus.</article-title>             <source>Cereb Cortex</source>             <volume>15</volume>             <issue>(5)</issue>             <fpage>654</fpage>             <lpage>662</lpage>          </element-citation></ref>
<ref id="pone.0003892-Corbetta1"><label>49</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Corbetta</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Shulman</surname><given-names>GL</given-names></name>
</person-group>             <year>2002</year>             <article-title>Control of goal-directed and stimulus-driven attention in the brain.</article-title>             <source>Nat Rev Neurosci</source>             <volume>3</volume>             <issue>(3)</issue>             <fpage>201</fpage>             <lpage>215</lpage>          </element-citation></ref>
<ref id="pone.0003892-Reddy1"><label>50</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Reddy</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Kanwisher</surname><given-names>N</given-names></name>
</person-group>             <year>2007</year>             <article-title>Category selectivity in the ventral visual pathway confers robustness to clutter and diverted attention.</article-title>             <source>Curr Biol</source>             <volume>17</volume>             <issue>(23)</issue>             <fpage>2067</fpage>             <lpage>2072</lpage>          </element-citation></ref>
<ref id="pone.0003892-Fries1"><label>51</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Fries</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Reynolds</surname><given-names>JH</given-names></name>
<name name-style="western"><surname>Rorie</surname><given-names>AE</given-names></name>
<name name-style="western"><surname>Desimone</surname><given-names>R</given-names></name>
</person-group>             <year>2001</year>             <article-title>Modulation of oscillatory neuronal synchronization by selective visual attention.</article-title>             <source>Science</source>             <volume>291</volume>             <issue>(5508)</issue>             <fpage>1560</fpage>             <lpage>1563</lpage>          </element-citation></ref>
<ref id="pone.0003892-Womelsdorf1"><label>52</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Womelsdorf</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Fries</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Mitra</surname><given-names>PP</given-names></name>
<name name-style="western"><surname>Desimone</surname><given-names>R</given-names></name>
</person-group>             <year>2006</year>             <article-title>Gamma-band synchronization in visual cortex predicts speed of change detection.</article-title>             <source>Nature</source>             <volume>439</volume>             <issue>(7077)</issue>             <fpage>733</fpage>             <lpage>736</lpage>          </element-citation></ref>
<ref id="pone.0003892-Averbeck1"><label>53</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Averbeck</surname><given-names>BB</given-names></name>
<name name-style="western"><surname>Latham</surname><given-names>PE</given-names></name>
<name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name>
</person-group>             <year>2006</year>             <article-title>Neural correlations, population coding and computation.</article-title>             <source>Nat Rev Neurosci</source>             <volume>7</volume>             <issue>(5)</issue>             <fpage>358</fpage>             <lpage>366</lpage>          </element-citation></ref>
<ref id="pone.0003892-Kriegeskorte1"><label>54</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kriegeskorte</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Bandettini</surname><given-names>P</given-names></name>
</person-group>             <year>2007</year>             <article-title>Analyzing for information, not activation, to exploit high-resolution fMRI.</article-title>             <source>Neuroimage</source>             <volume>38</volume>             <issue>(4)</issue>             <fpage>649</fpage>             <lpage>662</lpage>          </element-citation></ref>
<ref id="pone.0003892-Kriegeskorte2"><label>55</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kriegeskorte</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Goebel</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Bandettini</surname><given-names>P</given-names></name>
</person-group>             <year>2006</year>             <article-title>Information-based functional brain mapping.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>103</volume>             <issue>(10)</issue>             <fpage>3863</fpage>             <lpage>3868</lpage>          </element-citation></ref>
<ref id="pone.0003892-Wu1"><label>56</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wu</surname><given-names>MC</given-names></name>
<name name-style="western"><surname>David</surname><given-names>SV</given-names></name>
<name name-style="western"><surname>Gallant</surname><given-names>JL</given-names></name>
</person-group>             <year>2006</year>             <article-title>Complete functional characterization of sensory neurons by system identification.</article-title>             <source>Annu Rev Neurosci</source>             <volume>29</volume>             <fpage>477</fpage>             <lpage>505</lpage>          </element-citation></ref>
<ref id="pone.0003892-Ekman1"><label>57</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Ekman</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Friesen</surname><given-names>WV</given-names></name>
</person-group>             <year>1976</year>             <source>Pictures of Facial Affect</source>             <publisher-loc>Palo Alto, CA</publisher-loc>             <publisher-name>Consulting Psychologist Press</publisher-name>          </element-citation></ref>
<ref id="pone.0003892-Brainard1"><label>58</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Brainard</surname><given-names>DH</given-names></name>
</person-group>             <year>1997</year>             <article-title>The Psychophysics Toolbox.</article-title>             <source>Spat Vis</source>             <volume>10</volume>             <issue>(4)</issue>             <fpage>433</fpage>             <lpage>436</lpage>          </element-citation></ref>
<ref id="pone.0003892-Benjamini1"><label>59</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Benjamini</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Hochberg</surname><given-names>Y</given-names></name>
</person-group>             <year>1995</year>             <article-title>Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing.</article-title>             <source>Journal of the Royal Statistical Society Series B (Methodological)</source>             <volume>57</volume>             <issue>(1)</issue>             <fpage>289</fpage>             <lpage>300</lpage>          </element-citation></ref>
</ref-list>

</back>
</article>